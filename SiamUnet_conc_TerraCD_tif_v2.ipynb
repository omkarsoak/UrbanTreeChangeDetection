{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FHV3km-tCqoU",
        "outputId": "11fc4ebc-c906-48ba-d915-5411c3173eb2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip '/content/drive/MyDrive/BTechProject/ChangeDetectionMergedDividedSplit-tif3.zip' -d '/content/ChangeDetectionMergedDividedSplit-tif'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMowMJXZChmE"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgNN7EKOVJiY",
        "outputId": "c2d6b7fd-3bd3-44a3-95f3-2088556eb7a3"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRKZrxCtChmG",
        "outputId": "30a3cbc0-fd5a-444e-d109-fe87d4fbbc9d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import rasterio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class ChangeDetectionDatasetTIF(Dataset):\n",
        "    def __init__(self, t2019_dir, t2024_dir, mask_dir,classes, transform=None):\n",
        "        self.t2019_dir = t2019_dir\n",
        "        self.t2024_dir = t2024_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.classes = classes  # Change detection classes\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load all paths\n",
        "        self.t2019_paths = sorted([f for f in os.listdir(t2019_dir) if f.endswith('.tif')])\n",
        "        self.t2024_paths = sorted([f for f in os.listdir(t2024_dir) if f.endswith('.tif')])\n",
        "        self.mask_paths = sorted([f for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.t2019_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load images using rasterio\n",
        "        with rasterio.open(os.path.join(self.t2019_dir, self.t2019_paths[index])) as src:\n",
        "            img_t2019 = src.read(out_dtype=np.float32) / 255.0\n",
        "        with rasterio.open(os.path.join(self.t2024_dir, self.t2024_paths[index])) as src:\n",
        "            img_t2024 = src.read(out_dtype=np.float32) / 255.0\n",
        "        # Load masks\n",
        "        with rasterio.open(os.path.join(self.mask_dir, self.mask_paths[index])) as src:\n",
        "            cd_mask = src.read(1).astype(np.int64)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        img_t2019 = torch.from_numpy(img_t2019)\n",
        "        img_t2024 = torch.from_numpy(img_t2024)\n",
        "        cd_mask = torch.from_numpy(cd_mask)\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform is not None:\n",
        "            img_t2019 = self.transform(img_t2019)\n",
        "            img_t2024 = self.transform(img_t2024)\n",
        "\n",
        "        return img_t2019, img_t2024, cd_mask\n",
        "\n",
        "def describe_loader(loader_type):\n",
        "    img2019, img2024, cd_mask = next(iter(loader_type))\n",
        "    print(\"Batch size:\", loader_type.batch_size)\n",
        "    print(\"2019 Image Shape:\", img2019.shape)\n",
        "    print(\"2024 Image Shape:\", img2024.shape)\n",
        "    print(\"Change Mask Shape:\", cd_mask.shape)\n",
        "    print(\"Number of images:\", len(loader_type.dataset))\n",
        "    print(\"Classes:\", loader_type.dataset.classes)\n",
        "    print(\"Unique CD values:\", torch.unique(cd_mask))\n",
        "\n",
        "# Example usage:\n",
        "ROOT_DIRECTORY = \"ChangeDetectionMergedDividedSplit-tif\"\n",
        "SAVING_DIR = \"/content/drive/MyDrive/BTechProject\"\n",
        "CD_DIR = \"cd2_Output\"\n",
        "#CLASSES = ['no_change','vegetation_increase','vegetation_decrease']\n",
        "CLASSES = ['no_change', 'water_building', 'water_sparse', 'water_dense',\n",
        "           'building_water', 'building_sparse', 'building_dense',\n",
        "           'sparse_water', 'sparse_building', 'sparse_dense',\n",
        "           'dense_water', 'dense_building', 'dense_sparse']\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/train/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/train/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/train/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "val_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/val/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/val/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/val/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "test_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/test/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/test/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/test/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "num_workers = 8\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,num_workers=num_workers, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(\"------------Train-----------\")\n",
        "describe_loader(train_loader)\n",
        "print(\"------------Val------------\")\n",
        "describe_loader(val_loader)\n",
        "print(\"------------Test------------\")\n",
        "describe_loader(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpBgPG9nChmH"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "EkAWjfmNChmH",
        "outputId": "17007af8-008a-4a98-a517-b2249861f5be"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set up the plot size and remove axes\n",
        "fig, axs = plt.subplots(5, 3, figsize=(10,10))\n",
        "\n",
        "for i in range(5):\n",
        "    j = random.randint(0, len(train_dataset) - 1)\n",
        "    image1, image2, mask = train_dataset[j]\n",
        "\n",
        "    # Display images\n",
        "    axs[i, 0].imshow(image1.permute(1, 2, 0))\n",
        "    axs[i, 0].set_title(f\"Real 2019\")\n",
        "    axs[i, 0].axis(\"off\")\n",
        "\n",
        "    axs[i, 1].imshow(image2.permute(1, 2, 0))\n",
        "    axs[i, 1].set_title(f\"Real 2024\")\n",
        "    axs[i, 1].axis(\"off\")\n",
        "\n",
        "    axs[i, 2].imshow(mask, cmap=\"turbo\")\n",
        "    print(np.unique(mask))\n",
        "    axs[i, 2].set_title(f\"CD Mask\")\n",
        "    axs[i, 2].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKI0wDXtChmI"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7C2m2w0E3Rb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.padding import ReplicationPad2d\n",
        "import torch.optim as optim\n",
        "import json\n",
        "import os\n",
        "class SiamUnet_conc(nn.Module):\n",
        "    \"\"\"SiamUnet_conc segmentation network for multiclass change detection.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(SiamUnet_conc, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr\n",
        "        self.label_nbr = label_nbr  # Added for clarity\n",
        "\n",
        "        # Encoder Layers\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        # Decoder Layers\n",
        "        self.conv43d = nn.ConvTranspose2d(384, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(192, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(96, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(48, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        # Changed to use label_nbr instead of hardcoded value\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        # Multiclass activation (Softmax instead of LogSoftmax)\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
        "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
        "\n",
        "        ####################################################\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
        "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
        "\n",
        "        ####################################################\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), x43_1, x43_2), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), x33_1, x33_2), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), x22_1, x22_2), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), x12_1, x12_2), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0-zY8rUE3Rc"
      },
      "source": [
        "## Util Functions and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Mk8d2vUXwH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "def calculate_effective_weights(train_loader, device, num_classes=3, method='square_balanced'):\n",
        "    \"\"\"Calculate class weights with different strategies to handle class imbalance\n",
        "\n",
        "    Args:\n",
        "        train_loader: DataLoader containing training data\n",
        "        device: torch device\n",
        "        num_classes: number of classes (default: 3)\n",
        "        method: weighting strategy ('balanced', 'square_balanced', or 'custom')\n",
        "    \"\"\"\n",
        "    class_counts = torch.zeros(num_classes)\n",
        "    total_pixels = 0\n",
        "\n",
        "    # Count class frequencies\n",
        "    for _, _, labels in train_loader:\n",
        "        labels = labels.to(device)\n",
        "        for i in range(num_classes):\n",
        "            class_counts[i] += (labels == i).sum().item()\n",
        "        total_pixels += labels.numel()\n",
        "\n",
        "    class_frequencies = class_counts / total_pixels\n",
        "\n",
        "    if method == 'balanced':\n",
        "        # Standard balanced weighting (inverse frequency)\n",
        "        weights = 1.0 / class_frequencies\n",
        "\n",
        "    elif method == 'square_balanced':\n",
        "        # Square root of inverse frequencies (less aggressive balancing)\n",
        "        weights = torch.sqrt(1.0 / class_frequencies)\n",
        "\n",
        "    elif method == 'custom':\n",
        "        # Custom weighting that maintains some natural class distribution\n",
        "        # Adjust these factors based on your domain knowledge\n",
        "        base_weights = 1.0 / class_frequencies\n",
        "        adjustment_factors = torch.tensor([0.7, 1.2, 1.2])\n",
        "        weights = base_weights * adjustment_factors\n",
        "\n",
        "    # Normalize weights to sum to num_classes\n",
        "    weights = weights * (num_classes / weights.sum())\n",
        "\n",
        "    return weights, class_frequencies\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def calculate_metrics(outputs, labels, num_classes=3):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive metrics for change detection using a single confusion matrix\n",
        "\n",
        "    Args:\n",
        "        outputs (torch.Tensor or np.array): Model outputs or predictions\n",
        "        labels (torch.Tensor or np.array): Ground truth class labels\n",
        "        num_classes (int): Number of classes in the dataset\n",
        "\n",
        "    Returns:\n",
        "        list: List of overall performance metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to numpy if inputs are torch tensors\n",
        "    if torch.is_tensor(outputs):\n",
        "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    else:\n",
        "        predictions = outputs\n",
        "\n",
        "    if torch.is_tensor(labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Flatten predictions and targets\n",
        "    pred_flat = predictions.flatten()\n",
        "    target_flat = labels.flatten()\n",
        "\n",
        "    # Compute confusion matrix once\n",
        "    cm = confusion_matrix(target_flat, pred_flat, labels=list(range(num_classes)))\n",
        "\n",
        "    # Calculate metrics from confusion matrix\n",
        "    metrics = {}\n",
        "\n",
        "    # True positives, false positives, false negatives for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = np.sum(cm, axis=0) - tp\n",
        "    fn = np.sum(cm, axis=1) - tp\n",
        "\n",
        "    # Overall accuracy from confusion matrix\n",
        "    metrics['accuracy'] = np.sum(tp) / np.sum(cm)\n",
        "\n",
        "    # Per-class precision, recall, F1\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall = tp / (tp + fn + 1e-6)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
        "\n",
        "    # Unweighted averages\n",
        "    metrics['precision'] = np.average(precision) \n",
        "    metrics['recall'] = np.average(recall)\n",
        "    metrics['f1_score'] = np.average(f1)\n",
        "\n",
        "    # Weighted averages\n",
        "    # total = np.sum(cm, axis=1)\n",
        "    # metrics['precision'] = np.average(precision,weights=total)\n",
        "    # metrics['recall'] = np.average(recall, weights=total)\n",
        "    # metrics['f1_score'] = np.average(f1,weights=total)\n",
        "\n",
        "    # Calculate Kappa directly from confusion matrix\n",
        "    n = np.sum(cm)\n",
        "    sum_po = np.sum(np.diag(cm))\n",
        "    sum_pe = np.sum(np.sum(cm, axis=0) * np.sum(cm, axis=1)) / n\n",
        "    metrics['kappa'] = (sum_po - sum_pe) / (n - sum_pe + 1e-6)\n",
        "\n",
        "    # IoU from confusion matrix\n",
        "    iou_per_class = tp / (tp + fp + fn + 1e-6)\n",
        "    metrics['miou'] = np.mean(iou_per_class)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def train_model_balanced(model, train_loader, val_loader, num_epochs=50, num_classes=3, \n",
        "                         device='cuda', weighting_method='square_balanced',\n",
        "                         checkpoint_path='best_model_multiclass.pt'):\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Initialize history dictionary\n",
        "    def init_phase_metrics():\n",
        "        return {\n",
        "            'loss': [],\n",
        "            'accuracy': [],\n",
        "            'precision': [],\n",
        "            'recall': [],\n",
        "            'f1_score': [],\n",
        "            'miou': [],\n",
        "            'kappa': []\n",
        "        }\n",
        "\n",
        "    history = {\n",
        "        'train': init_phase_metrics(),\n",
        "        'val': init_phase_metrics()\n",
        "    }\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            best_val_loss = checkpoint['best_val_loss']\n",
        "            print(f\"Resuming from epoch {start_epoch} with best val loss: {best_val_loss:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting training from scratch\")\n",
        "\n",
        "    # Setup model, optimizer, criterion\n",
        "    class_weights, _ = calculate_effective_weights(train_loader, device, num_classes=num_classes, method=weighting_method)\n",
        "    print(class_weights)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    \n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "    model.to(device)\n",
        "\n",
        "    def process_epoch(phase, data_loader):\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_metrics = {\n",
        "            'loss': 0.0,\n",
        "            'accuracy': 0.0,\n",
        "            'precision': 0.0,\n",
        "            'recall': 0.0,\n",
        "            'f1_score': 0.0,\n",
        "            'miou': 0.0,\n",
        "            'kappa': 0.0\n",
        "        }\n",
        "        samples_count = 0\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "            for inputs1, inputs2, labels in data_loader:\n",
        "                inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
        "                batch_size = inputs1.size(0)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs1, inputs2)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Calculate metrics\n",
        "                metrics = calculate_metrics(outputs, labels, num_classes=num_classes)\n",
        "                metrics['loss'] = loss.item()\n",
        "\n",
        "                # Update running metrics\n",
        "                for key in running_metrics:\n",
        "                    running_metrics[key] += metrics[key] * batch_size\n",
        "                samples_count += batch_size\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_metrics = {key: value / samples_count for key, value in running_metrics.items()}\n",
        "\n",
        "        # Store metrics in history\n",
        "        for key in history[phase]:\n",
        "            history[phase][key].append(epoch_metrics[key])\n",
        "\n",
        "        return epoch_metrics\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        print(f'\\nEpoch {epoch + 1}/{num_epochs}:')\n",
        "\n",
        "        # Training phase\n",
        "        train_metrics = process_epoch('train', train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = process_epoch('val', val_loader)\n",
        "\n",
        "        # Print metrics\n",
        "        def print_metrics(phase, metrics):\n",
        "            print(f'\\n{phase.capitalize()} Metrics:')\n",
        "            print(f'  Loss: {metrics[\"loss\"]:.4f}')\n",
        "            print(f'  Accuracy: {metrics[\"accuracy\"]:.4f}')\n",
        "            print(f'  Precision: {metrics[\"precision\"]:.4f}')\n",
        "            print(f'  Recall: {metrics[\"recall\"]:.4f}')\n",
        "            print(f'  F1-score: {metrics[\"f1_score\"]:.4f}')\n",
        "            print(f'  mIoU: {metrics[\"miou\"]:.4f}')\n",
        "            print(f'  Kappa: {metrics[\"kappa\"]:.4f}')\n",
        "\n",
        "        print_metrics('train', train_metrics)\n",
        "        print_metrics('val', val_metrics)\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(val_metrics['loss'])\n",
        "\n",
        "        # Save checkpoint if it's the best model\n",
        "        if val_metrics['loss'] < best_val_loss:\n",
        "            best_val_loss = val_metrics['loss']\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'metrics': val_metrics,\n",
        "                'history': history\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f'\\nSaved new best model with validation loss: {val_metrics[\"loss\"]:.4f}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def save_training_files(history, checkpoint_path, history_filename, bestepoch_filename):\n",
        "    \"\"\"Save training history and best epoch info to separate JSON files\"\"\"\n",
        "\n",
        "    def convert_to_serializable(value):\n",
        "        \"\"\"Recursively convert numpy/torch types to basic Python types\"\"\"\n",
        "        if isinstance(value, (np.ndarray, torch.Tensor)):\n",
        "            return value.tolist()\n",
        "        elif isinstance(value, dict):\n",
        "            return {k: convert_to_serializable(v) for k, v in value.items()}\n",
        "        elif isinstance(value, list):\n",
        "            return [convert_to_serializable(item) for item in value]\n",
        "        return value\n",
        "\n",
        "    history_data = {\n",
        "        phase: {\n",
        "            metric: convert_to_serializable(values)\n",
        "            for metric, values in metrics.items()\n",
        "        }\n",
        "        for phase, metrics in history.items()\n",
        "    }\n",
        "\n",
        "    with open(history_filename, 'w') as f:\n",
        "        json.dump(history_data, f, indent=4)\n",
        "\n",
        "    # Load checkpoint without weights_only flag\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    # print(\"\\nCheckpoint contents:\")\n",
        "    # for key in checkpoint.keys():\n",
        "    #     print(f\"- {key}\")\n",
        "\n",
        "    # Convert metrics to basic Python types\n",
        "    epoch_data = {\n",
        "        'best_epoch': checkpoint['epoch'],\n",
        "        'best_val_loss': checkpoint['best_val_loss'],\n",
        "        'val_metrics': convert_to_serializable(checkpoint['metrics'])\n",
        "    }\n",
        "\n",
        "    with open(bestepoch_filename, 'w') as f:\n",
        "        json.dump(epoch_data, f, indent=4)\n",
        "\n",
        "    print(f\"\\nSaved training history to: {history_filename}\")\n",
        "    print(f\"Saved best epoch info to: {bestepoch_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2RI6l8bUXwH"
      },
      "source": [
        "## Model Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EayeD9l0u2V",
        "outputId": "dcabdf15-de5c-4889-ba24-2aba5ebed816"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = 'siamunet_conc'\n",
        "strategy = 'st2' #change detection strategy {1,2,3,4}\n",
        "num_classes = 13  #num classes in change mask\n",
        "num_epochs = 2\n",
        "weighting_method = 'square_balanced' #'custom'\n",
        "\n",
        "checkpoint_path = f'{SAVING_DIR}/best_{strategy}_{model_name}-{num_classes}_classes_{num_epochs}.pt'\n",
        "\n",
        "model = SiamUnet_conc(input_nbr=3, label_nbr=num_classes).to(device)\n",
        "model2, history = train_model_balanced(model, train_loader, val_loader,\n",
        "                                      num_epochs=num_epochs, num_classes=num_classes,\n",
        "                                      device=device,\n",
        "                                      weighting_method=weighting_method,\n",
        "                                      checkpoint_path=checkpoint_path)\n",
        "\n",
        "\n",
        "history_filename = f\"{SAVING_DIR}/{strategy}_{model_name}-{num_classes}_classes_{num_epochs}_history.json\"\n",
        "bestepoch_filename = f\"{SAVING_DIR}/{strategy}_{model_name}-{num_classes}_classes_{num_epochs}_best_epoch.json\"\n",
        "save_training_files(history=history,checkpoint_path=checkpoint_path,\n",
        "                    history_filename=history_filename,bestepoch_filename=bestepoch_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa0RAqCdChmI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i6ngPdKERbCE",
        "outputId": "beaa54e4-8736-4aeb-ed62-3329a1c1e832"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "def test_model(model, test_loader, device='cuda',\n",
        "               num_classes=3, weighting_method='square_balanced'):\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights, _ = calculate_effective_weights(test_loader, device,\n",
        "                                                   num_classes=num_classes,\n",
        "                                                   method=weighting_method)\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "    # Select loss function\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "    # For visualization and metrics\n",
        "    random_samples = []\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Collect predictions and labels for comprehensive metrics\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs1, inputs2, labels in test_loader:\n",
        "            inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs1, inputs2)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item() * inputs1.size(0)\n",
        "            total_samples += inputs1.size(0)\n",
        "\n",
        "            # Get predictions\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Store predictions and labels\n",
        "            all_predictions.append(preds.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "            # Store random samples for visualization\n",
        "            if len(random_samples) < 5:\n",
        "                for i in range(min(inputs1.size(0), 5 - len(random_samples))):\n",
        "                    if random.random() < 0.2:  # 20% chance to select each sample\n",
        "                        random_samples.append({\n",
        "                            'image1': inputs1[i].cpu(),\n",
        "                            'image2': inputs2[i].cpu(),\n",
        "                            'label': labels[i].cpu(),\n",
        "                            'pred': preds[i].cpu(),\n",
        "                            'probabilities': torch.softmax(outputs[i], dim=0).cpu()\n",
        "                        })\n",
        "\n",
        "    # Concatenate predictions and labels\n",
        "    all_predictions = np.concatenate(all_predictions)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_metrics = calculate_metrics(all_predictions, all_labels, num_classes)\n",
        "\n",
        "    # Add loss to metrics\n",
        "    test_metrics['loss'] = total_loss / total_samples\n",
        "\n",
        "    # Make sure we have exactly 5 samples\n",
        "    while len(random_samples) < 5:\n",
        "        random_samples.append(random_samples[-1] if random_samples else {\n",
        "            'image1': torch.zeros(3, 64, 64),\n",
        "            'image2': torch.zeros(3, 64, 64),\n",
        "            'label': torch.zeros(64, 64),\n",
        "            'pred': torch.zeros(64, 64),\n",
        "            'probabilities': torch.zeros(3, 64, 64)\n",
        "        })\n",
        "\n",
        "    return random_samples, test_metrics\n",
        "\n",
        "def visualize_results(random_samples, num_classes=3):\n",
        "    # Create a figure with subplots\n",
        "    fig, axes = plt.subplots(5, 4, figsize=(25, 25))\n",
        "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "    for idx, sample in enumerate(random_samples):\n",
        "        # Normalize and convert images for display\n",
        "        img1 = sample['image1'].numpy().transpose(1, 2, 0)\n",
        "        img2 = sample['image2'].numpy().transpose(1, 2, 0)\n",
        "        img1 = (img1 - img1.min()) / (img1.max() - img1.min())\n",
        "        img2 = (img2 - img2.min()) / (img2.max() - img2.min())\n",
        "\n",
        "        # Get masks\n",
        "        pred_mask = sample['pred'].numpy()\n",
        "        true_mask = sample['label'].numpy()\n",
        "\n",
        "        # Plot images and masks\n",
        "        axes[idx, 0].imshow(img1)\n",
        "        axes[idx, 0].set_title('Image 1')\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        axes[idx, 1].imshow(img2)\n",
        "        axes[idx, 1].set_title('Image 2')\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Plot predicted mask\n",
        "        pred_plot = axes[idx, 2].imshow(pred_mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
        "        axes[idx, 2].set_title('Predicted Change')\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "        # Plot ground truth mask\n",
        "        true_plot = axes[idx, 3].imshow(true_mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
        "        axes[idx, 3].set_title('Ground Truth')\n",
        "        axes[idx, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_test_metrics(test_metrics, save_dir, model_name, strategy, num_epochs):\n",
        "    \"\"\"Save test metrics to JSON\"\"\"\n",
        "    metrics_file = os.path.join(save_dir, f\"{strategy}_{model_name}-{num_classes}_classes_{num_epochs}_test_metrics.json\")\n",
        "\n",
        "    # Use the pre-computed metrics directly\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        json.dump(test_metrics, f, indent=4)\n",
        "\n",
        "    print(f\"\\nSaved test metrics to: {metrics_file}\")\n",
        "\n",
        "# Test the model\n",
        "random_samples, test_metrics = test_model(model, test_loader, device=device, \n",
        "                                          num_classes=num_classes)\n",
        "\n",
        "# Save test metrics\n",
        "save_test_metrics(test_metrics=test_metrics,\n",
        "                  save_dir=SAVING_DIR,\n",
        "                  model_name=model_name,\n",
        "                  strategy=strategy,\n",
        "                  num_epochs=num_epochs)\n",
        "\n",
        "# Visualize results\n",
        "visualize_results(random_samples,num_classes=num_classes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5939633,
          "sourceId": 9710583,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
