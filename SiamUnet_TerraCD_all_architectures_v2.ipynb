{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FHV3km-tCqoU",
        "outputId": "85773638-7f22-4ccf-fb95-050da37b947a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip '/content/drive/MyDrive/BTechProject/ChangeDetectionMergedDividedSplit-DW-tif.zip' -d '/content/ChangeDetectionMergedDividedSplit-tif'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgNN7EKOVJiY",
        "outputId": "ddbabcd3-47a5-417a-d482-78a663bdfc8b"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RApcrFLIwWcA"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo1UdGCuwWcD"
      },
      "outputs": [],
      "source": [
        "ROOT_DIRECTORY = \"ChangeDetectionMergedDividedSplit-tif\"\n",
        "SAVING_DIR = \"/content/drive/MyDrive/BTechProject\"\n",
        "CD_DIR = \"cd1_Output\"\n",
        "\n",
        "if CD_DIR == \"cd1_Output\":\n",
        "    CLASSES = ['no_change','vegetation_increase','vegetation_decrease']\n",
        "elif CD_DIR == \"cd2_Output\":\n",
        "    # CLASSES = ['no_change', 'water_building', 'water_sparse', 'water_dense',\n",
        "    #            'building_water', 'building_sparse', 'building_dense',\n",
        "    #            'sparse_water', 'sparse_building', 'sparse_dense',\n",
        "    #            'dense_water', 'dense_building', 'dense_sparse']\n",
        "    CLASSES = [\n",
        "    'no_change','water_built', 'water_bare', 'water_sparse', 'water_trees',\n",
        "    'water_crops', 'built_water', 'built_bare', 'built_sparse', 'built_trees',\n",
        "    'built_crops',  'bare_water',  'bare_built',  'bare_sparse',  'bare_trees',\n",
        "    'bare_crops',  'sparse_water',  'sparse_built',  'sparse_bare',\n",
        "    'sparse_trees',  'sparse_crops',  'trees_water',  'trees_built',\n",
        "    'trees_bare',  'trees_sparse',  'trees_crops',  'crops_water',\n",
        "    'crops_built', 'crops_bare',  'crops_sparse',  'crops_trees']\n",
        "\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 5\n",
        "MODEL_NAME = 'siamunet_conc'\n",
        "#'siamunet_conc','siamunet_diff','siamunet_EF','snunet_conc','snunet_ECAM'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMowMJXZChmE"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRKZrxCtChmG",
        "outputId": "dd441ebd-279c-487b-bc2c-2c34110702c3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import rasterio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class ChangeDetectionDatasetTIF(Dataset):\n",
        "    def __init__(self, t2019_dir, t2024_dir, mask_dir,classes, transform=None):\n",
        "        self.t2019_dir = t2019_dir\n",
        "        self.t2024_dir = t2024_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.classes = classes  # Change detection classes\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load all paths\n",
        "        self.t2019_paths = sorted([f for f in os.listdir(t2019_dir) if f.endswith('.tif')])\n",
        "        self.t2024_paths = sorted([f for f in os.listdir(t2024_dir) if f.endswith('.tif')])\n",
        "        self.mask_paths = sorted([f for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.t2019_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load images using rasterio\n",
        "        with rasterio.open(os.path.join(self.t2019_dir, self.t2019_paths[index])) as src:\n",
        "            img_t2019 = src.read(out_dtype=np.float32) / 255.0\n",
        "        with rasterio.open(os.path.join(self.t2024_dir, self.t2024_paths[index])) as src:\n",
        "            img_t2024 = src.read(out_dtype=np.float32) / 255.0\n",
        "        # Load masks\n",
        "        with rasterio.open(os.path.join(self.mask_dir, self.mask_paths[index])) as src:\n",
        "            cd_mask = src.read(1).astype(np.int64)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        img_t2019 = torch.from_numpy(img_t2019)\n",
        "        img_t2024 = torch.from_numpy(img_t2024)\n",
        "        cd_mask = torch.from_numpy(cd_mask)\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform is not None:\n",
        "            img_t2019 = self.transform(img_t2019)\n",
        "            img_t2024 = self.transform(img_t2024)\n",
        "\n",
        "        return img_t2019, img_t2024, cd_mask\n",
        "\n",
        "def describe_loader(loader_type):\n",
        "    img2019, img2024, cd_mask = next(iter(loader_type))\n",
        "    print(\"Batch size:\", loader_type.batch_size)\n",
        "    print(\"2019 Image Shape:\", img2019.shape)\n",
        "    print(\"2024 Image Shape:\", img2024.shape)\n",
        "    print(\"Change Mask Shape:\", cd_mask.shape)\n",
        "    print(\"Number of images:\", len(loader_type.dataset))\n",
        "    print(\"Classes:\", loader_type.dataset.classes)\n",
        "    print(\"Unique CD values:\", torch.unique(cd_mask))\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/train/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/train/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/train/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "val_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/val/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/val/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/val/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "test_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/test/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/test/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/test/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(\"------------Train-----------\")\n",
        "describe_loader(train_loader)\n",
        "print(\"------------Val------------\")\n",
        "describe_loader(val_loader)\n",
        "print(\"------------Test------------\")\n",
        "describe_loader(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpBgPG9nChmH"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "EkAWjfmNChmH",
        "outputId": "81233891-60fd-4fd0-a2c7-8f6e1e7b22e6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set up the plot size and remove axes\n",
        "fig, axs = plt.subplots(5, 3, figsize=(10,10))\n",
        "\n",
        "for i in range(5):\n",
        "    j = random.randint(0, len(train_dataset) - 1)\n",
        "    image1, image2, mask = train_dataset[j]\n",
        "\n",
        "    # Display images\n",
        "    axs[i, 0].imshow(image1.permute(1, 2, 0))\n",
        "    axs[i, 0].set_title(f\"Real 2019\")\n",
        "    axs[i, 0].axis(\"off\")\n",
        "\n",
        "    axs[i, 1].imshow(image2.permute(1, 2, 0))\n",
        "    axs[i, 1].set_title(f\"Real 2024\")\n",
        "    axs[i, 1].axis(\"off\")\n",
        "\n",
        "    axs[i, 2].imshow(mask, cmap=\"turbo\")\n",
        "    print(np.unique(mask))\n",
        "    axs[i, 2].set_title(f\"CD Mask\")\n",
        "    axs[i, 2].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKI0wDXtChmI"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7C2m2w0E3Rb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.padding import ReplicationPad2d\n",
        "import torch.optim as optim\n",
        "\n",
        "########## SiamUnet_conc ##########\n",
        "class SiamUnet_conc(nn.Module):\n",
        "    \"\"\"SiamUnet_conc segmentation network for multiclass change detection.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(SiamUnet_conc, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr\n",
        "        self.label_nbr = label_nbr  # Added for clarity\n",
        "\n",
        "        # Encoder Layers\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        # Decoder Layers\n",
        "        self.conv43d = nn.ConvTranspose2d(384, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(192, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(96, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(48, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        # Changed to use label_nbr instead of hardcoded value\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        # Multiclass activation (Softmax instead of LogSoftmax)\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
        "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
        "\n",
        "        ####################################################\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
        "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
        "\n",
        "        ####################################################\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), x43_1, x43_2), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), x33_1, x33_2), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), x22_1, x22_2), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), x12_1, x12_2), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)\n",
        "\n",
        "######### SiamUnet_diff #########\n",
        "class SiamUnet_diff(nn.Module):\n",
        "    \"\"\"SiamUnet_diff segmentation network for multiclass change detection.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(SiamUnet_diff, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr\n",
        "        self.label_nbr = label_nbr\n",
        "\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        # Changed to output label_nbr channels for multiclass\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        # Changed from LogSoftmax to regular Softmax\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
        "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
        "\n",
        "        ####################################################\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
        "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)\n",
        "\n",
        "######### SiamUnet_EF #########\n",
        "class SiamUnet_EF(nn.Module):\n",
        "    \"\"\"Multiclass UNet segmentation network for change detection.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(SiamUnet_EF, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr * 2  # Multiply by 2 since we concatenate two images\n",
        "        self.label_nbr = label_nbr\n",
        "\n",
        "        # Encoder Layers\n",
        "        self.conv11 = nn.Conv2d(input_nbr * 2, 16, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        # Decoder Layers\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        # Change output layer to match number of classes\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        # Change to Softmax for multiclass\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Initial concatenation of input images (early fusion)\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "\n",
        "        # Stage 1 - Encoder\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n",
        "        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2 - Encoder\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3 - Encoder\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4 - Encoder\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4d - Decoder\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), x43), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d - Decoder\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), x33), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d - Decoder\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), x22), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d - Decoder\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), x12), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)\n",
        "\n",
        "########### SNUNet_conc ###########\n",
        "class conv_block_nested(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        identity = x\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        output = self.activation(x + identity)\n",
        "        return output\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2,\n",
        "                            mode='bilinear',\n",
        "                            align_corners=True) if bilinear else \\\n",
        "                 nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class Siam_NestedUNet_Conc(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3):\n",
        "        super(Siam_NestedUNet_Conc, self).__init__()\n",
        "        torch.nn.Module.dump_patches = True\n",
        "        n1 = 32     # Initial number of channels\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Encoder path for both images\n",
        "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        self.Up1_0 = up(filters[1])\n",
        "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        self.Up2_0 = up(filters[2])\n",
        "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        self.Up3_0 = up(filters[3])\n",
        "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "        self.Up4_0 = up(filters[4])\n",
        "\n",
        "        # Nested dense connections with batch norm\n",
        "        self.conv0_1 = conv_block_nested(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_1 = up(filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_1 = up(filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] * 2 + filters[4], filters[3], filters[3])\n",
        "        self.Up3_1 = up(filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_2 = up(filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2] * 3 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_2 = up(filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1] * 4 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_3 = up(filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0] * 5 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        # Add batch normalization to deep supervision outputs\n",
        "        self.final1 = nn.Sequential(\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.final2 = nn.Sequential(\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.final3 = nn.Sequential(\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.final4 = nn.Sequential(\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        # Final combination layer with better feature extraction\n",
        "        self.conv_final = nn.Sequential(\n",
        "            nn.Conv2d(out_ch * 4, filters[0], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Use Xavier/Glorot initialization for final layers\n",
        "                if m.kernel_size[0] == 1:\n",
        "                    nn.init.xavier_uniform_(m.weight)\n",
        "                else:\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, xA, xB):\n",
        "        # Encoder Path A\n",
        "        x0_0A = self.conv0_0(xA)\n",
        "        x1_0A = self.conv1_0(self.pool(x0_0A))\n",
        "        x2_0A = self.conv2_0(self.pool(x1_0A))\n",
        "        x3_0A = self.conv3_0(self.pool(x2_0A))\n",
        "\n",
        "        # Encoder Path B\n",
        "        x0_0B = self.conv0_0(xB)\n",
        "        x1_0B = self.conv1_0(self.pool(x0_0B))\n",
        "        x2_0B = self.conv2_0(self.pool(x1_0B))\n",
        "        x3_0B = self.conv3_0(self.pool(x2_0B))\n",
        "        x4_0B = self.conv4_0(self.pool(x3_0B))\n",
        "\n",
        "        # Nested Dense Connections and Decoder Path\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n",
        "\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n",
        "\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n",
        "\n",
        "        # Get outputs at different scales\n",
        "        output1 = self.final1(x0_1)\n",
        "        output2 = self.final2(x0_2)\n",
        "        output3 = self.final3(x0_3)\n",
        "        output4 = self.final4(x0_4)\n",
        "\n",
        "        # Combine outputs\n",
        "        output = self.conv_final(torch.cat([output1, output2, output3, output4], 1))\n",
        "\n",
        "        # Return logits without softmax for CrossEntropyLoss\n",
        "        return output\n",
        "\n",
        "########### SNUNet_ECAM ###########\n",
        "class conv_block_nested(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        identity = x\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        output = self.activation(x + identity)\n",
        "        return output\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2,\n",
        "                            mode='bilinear',\n",
        "                            align_corners=True) if bilinear else \\\n",
        "                 nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        reduced_channels = max(8, in_channels // ratio)\n",
        "        self.fc1 = nn.Conv2d(in_channels, reduced_channels, 1, bias=False)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Conv2d(reduced_channels, in_channels, 1, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
        "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class SNUNet_ECAM(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3):\n",
        "        super(SNUNet_ECAM, self).__init__()\n",
        "        n1 = 32\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 8]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Encoder\n",
        "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        self.Up1_0 = up(filters[1])\n",
        "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        self.Up2_0 = up(filters[2])\n",
        "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        self.Up3_0 = up(filters[3])\n",
        "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "        self.Up4_0 = up(filters[4])\n",
        "\n",
        "        # Decoder with correct input channels\n",
        "        self.conv0_1 = conv_block_nested(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_1 = up(filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_1 = up(filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] * 2 + filters[4], filters[3], filters[3])\n",
        "        self.Up3_1 = up(filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_2 = up(filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2] * 3 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_2 = up(filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1] * 4 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_3 = up(filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0] * 5 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        # Channel attention modules\n",
        "        self.ca = ChannelAttention(filters[0] * 4, ratio=16)\n",
        "        self.ca1 = ChannelAttention(filters[0], ratio=8)\n",
        "\n",
        "        # Final convolution\n",
        "        self.conv_final = nn.Sequential(\n",
        "            nn.Conv2d(filters[0] * 4, filters[0], kernel_size=1),\n",
        "            nn.BatchNorm2d(filters[0]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, xA, xB):\n",
        "        # Encoder path A\n",
        "        x0_0A = self.conv0_0(xA)\n",
        "        x1_0A = self.conv1_0(self.pool(x0_0A))\n",
        "        x2_0A = self.conv2_0(self.pool(x1_0A))\n",
        "        x3_0A = self.conv3_0(self.pool(x2_0A))\n",
        "\n",
        "        # Encoder path B\n",
        "        x0_0B = self.conv0_0(xB)\n",
        "        x1_0B = self.conv1_0(self.pool(x0_0B))\n",
        "        x2_0B = self.conv2_0(self.pool(x1_0B))\n",
        "        x3_0B = self.conv3_0(self.pool(x2_0B))\n",
        "        x4_0B = self.conv4_0(self.pool(x3_0B))\n",
        "\n",
        "        # Dense connections level 1\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n",
        "\n",
        "        # Dense connections level 2\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n",
        "\n",
        "        # Dense connections level 3\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n",
        "\n",
        "        # Final dense connections\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n",
        "\n",
        "        # Combine features\n",
        "        out = torch.cat([x0_1, x0_2, x0_3, x0_4], 1)\n",
        "        intra = torch.sum(torch.stack((x0_1, x0_2, x0_3, x0_4)), dim=0)\n",
        "\n",
        "        # Apply attention\n",
        "        ca1 = self.ca1(intra)\n",
        "        out = self.ca(out) * (out + ca1.repeat(1, 4, 1, 1))\n",
        "\n",
        "        # Final convolution\n",
        "        out = self.conv_final(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0-zY8rUE3Rc"
      },
      "source": [
        "## Util Functions and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Mk8d2vUXwH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def calculate_effective_weights(train_loader, device, num_cd_classes=3, method='square_balanced'):\n",
        "    \"\"\"Calculate class weights with different strategies to handle class imbalance\n",
        "\n",
        "    Args:\n",
        "        train_loader: DataLoader containing training data\n",
        "        device: torch device\n",
        "        num_cd_classes: number of classes (default: 3)\n",
        "        method: weighting strategy ('balanced', 'square_balanced', or 'custom')\n",
        "    \"\"\"\n",
        "    class_counts = torch.zeros(num_cd_classes)\n",
        "    total_pixels = 0\n",
        "\n",
        "    # Count class frequencies\n",
        "    for _, _, labels in train_loader:\n",
        "        labels = labels.to(device)\n",
        "        for i in range(num_cd_classes):\n",
        "            class_counts[i] += (labels == i).sum().item()\n",
        "        total_pixels += labels.numel()\n",
        "\n",
        "    class_frequencies = class_counts / total_pixels\n",
        "\n",
        "    if method == 'balanced':\n",
        "        # Standard balanced weighting (inverse frequency)\n",
        "        weights = 1.0 / class_frequencies\n",
        "\n",
        "    elif method == 'square_balanced':\n",
        "        # Square root of inverse frequencies (less aggressive balancing)\n",
        "        weights = torch.sqrt(1.0 / class_frequencies)\n",
        "\n",
        "    elif method == 'custom':\n",
        "        # Custom weighting that maintains some natural class distribution\n",
        "        # Adjust these factors based on your domain knowledge\n",
        "        base_weights = 1.0 / class_frequencies\n",
        "        adjustment_factors = torch.tensor([0.7, 1.2, 1.2])\n",
        "        weights = base_weights * adjustment_factors\n",
        "\n",
        "    # Normalize weights to sum to num_cd_classes\n",
        "    weights = weights * (num_cd_classes / weights.sum())\n",
        "\n",
        "    return weights, class_frequencies\n",
        "\n",
        "def calculate_metrics(outputs, labels, num_cd_classes=3, weighted_metrics=False):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive metrics for change detection using a single confusion matrix\n",
        "\n",
        "    Args:\n",
        "        outputs (torch.Tensor or np.array): Model outputs or predictions\n",
        "        labels (torch.Tensor or np.array): Ground truth class labels\n",
        "        num_cd_classes (int): Number of classes in the dataset\n",
        "\n",
        "    Returns:\n",
        "        list: List of overall performance metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to numpy if inputs are torch tensors\n",
        "    if torch.is_tensor(outputs):\n",
        "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    else:\n",
        "        predictions = outputs\n",
        "\n",
        "    if torch.is_tensor(labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Flatten predictions and targets\n",
        "    pred_flat = predictions.flatten()\n",
        "    target_flat = labels.flatten()\n",
        "\n",
        "    # Compute confusion matrix once\n",
        "    cm = confusion_matrix(target_flat, pred_flat, labels=list(range(num_cd_classes)))\n",
        "\n",
        "    # Calculate metrics from confusion matrix\n",
        "    metrics = {}\n",
        "\n",
        "    # True positives, false positives, false negatives for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = np.sum(cm, axis=0) - tp\n",
        "    fn = np.sum(cm, axis=1) - tp\n",
        "\n",
        "    # Overall accuracy from confusion matrix\n",
        "    metrics['accuracy'] = np.sum(tp) / np.sum(cm)\n",
        "\n",
        "    # Per-class precision, recall, F1\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall = tp / (tp + fn + 1e-6)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
        "\n",
        "    if weighted_metrics == False:\n",
        "        # Unweighted averages\n",
        "        metrics['precision'] = np.average(precision)\n",
        "        metrics['recall'] = np.average(recall)\n",
        "        metrics['f1_score'] = np.average(f1)\n",
        "    elif weighted_metrics == True:\n",
        "        # Weighted averages\n",
        "        total = np.sum(cm, axis=1)\n",
        "        metrics['precision'] = np.average(precision,weights=total)\n",
        "        metrics['recall'] = np.average(recall, weights=total)\n",
        "        metrics['f1_score'] = np.average(f1,weights=total)\n",
        "\n",
        "    # Calculate Kappa directly from confusion matrix\n",
        "    n = np.sum(cm)\n",
        "    sum_po = np.sum(np.diag(cm))\n",
        "    sum_pe = np.sum(np.sum(cm, axis=0) * np.sum(cm, axis=1)) / n\n",
        "    metrics['kappa'] = (sum_po - sum_pe) / (n - sum_pe + 1e-6)\n",
        "\n",
        "    # IoU from confusion matrix\n",
        "    iou_per_class = tp / (tp + fp + fn + 1e-6)\n",
        "    metrics['miou'] = np.mean(iou_per_class)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def train_model_balanced(model, train_loader, val_loader, num_epochs=50, num_cd_classes=3,\n",
        "                         device='cuda', weighting_method='square_balanced',\n",
        "                         checkpoint_path='best_model_multiclass.pt',\n",
        "                         weighted_metrics=False):\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Initialize history dictionary\n",
        "    def init_phase_metrics():\n",
        "        return {\n",
        "            'loss': [],\n",
        "            'accuracy': [],\n",
        "            'precision': [],\n",
        "            'recall': [],\n",
        "            'f1_score': [],\n",
        "            'miou': [],\n",
        "            'kappa': []\n",
        "        }\n",
        "\n",
        "    history = {\n",
        "        'train': init_phase_metrics(),\n",
        "        'val': init_phase_metrics()\n",
        "    }\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            best_val_loss = checkpoint['best_val_loss']\n",
        "            print(f\"Resuming from epoch {start_epoch} with best val loss: {best_val_loss:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting training from scratch\")\n",
        "\n",
        "    # Setup model, optimizer, criterion\n",
        "    class_weights, _ = calculate_effective_weights(train_loader, device,\n",
        "                                                   num_cd_classes=num_cd_classes,\n",
        "                                                   method=weighting_method)\n",
        "    print(class_weights)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "    model.to(device)\n",
        "\n",
        "    def process_epoch(phase, data_loader):\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_metrics = {\n",
        "            'loss': 0.0,\n",
        "            'accuracy': 0.0,\n",
        "            'precision': 0.0,\n",
        "            'recall': 0.0,\n",
        "            'f1_score': 0.0,\n",
        "            'miou': 0.0,\n",
        "            'kappa': 0.0\n",
        "        }\n",
        "        samples_count = 0\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "            for inputs1, inputs2, labels in data_loader:\n",
        "                inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
        "                batch_size = inputs1.size(0)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs1, inputs2)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Calculate metrics\n",
        "                metrics = calculate_metrics(outputs, labels, num_cd_classes=num_cd_classes,\n",
        "                                            weighted_metrics=weighted_metrics)\n",
        "                metrics['loss'] = loss.item()\n",
        "\n",
        "                # Update running metrics\n",
        "                for key in running_metrics:\n",
        "                    running_metrics[key] += metrics[key] * batch_size\n",
        "                samples_count += batch_size\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_metrics = {key: value / samples_count for key, value in running_metrics.items()}\n",
        "\n",
        "        # Store metrics in history\n",
        "        for key in history[phase]:\n",
        "            history[phase][key].append(epoch_metrics[key])\n",
        "\n",
        "        return epoch_metrics\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        print(f'\\nEpoch {epoch + 1}/{num_epochs}:')\n",
        "\n",
        "        # Training phase\n",
        "        train_metrics = process_epoch('train', train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = process_epoch('val', val_loader)\n",
        "\n",
        "        # Print metrics\n",
        "        def print_metrics(phase, metrics):\n",
        "            print(f'\\n{phase.capitalize()} Metrics:')\n",
        "            print(f'  Loss: {metrics[\"loss\"]:.4f}')\n",
        "            print(f'  Accuracy: {metrics[\"accuracy\"]:.4f}')\n",
        "            print(f'  Precision: {metrics[\"precision\"]:.4f}')\n",
        "            print(f'  Recall: {metrics[\"recall\"]:.4f}')\n",
        "            print(f'  F1-score: {metrics[\"f1_score\"]:.4f}')\n",
        "            print(f'  mIoU: {metrics[\"miou\"]:.4f}')\n",
        "            print(f'  Kappa: {metrics[\"kappa\"]:.4f}')\n",
        "\n",
        "        print_metrics('train', train_metrics)\n",
        "        print_metrics('val', val_metrics)\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(val_metrics['loss'])\n",
        "\n",
        "        # Save checkpoint if it's the best model\n",
        "        if val_metrics['loss'] < best_val_loss:\n",
        "            best_val_loss = val_metrics['loss']\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'metrics': val_metrics,\n",
        "                'history': history\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f'\\nSaved new best model with validation loss: {val_metrics[\"loss\"]:.4f}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def save_training_files(history, checkpoint_path, history_filename, bestepoch_filename):\n",
        "    \"\"\"Save training history and best epoch info to separate JSON files\"\"\"\n",
        "\n",
        "    def convert_to_serializable(value):\n",
        "        \"\"\"Recursively convert numpy/torch types to basic Python types\"\"\"\n",
        "        if isinstance(value, (np.ndarray, torch.Tensor)):\n",
        "            return value.tolist()\n",
        "        elif isinstance(value, dict):\n",
        "            return {k: convert_to_serializable(v) for k, v in value.items()}\n",
        "        elif isinstance(value, list):\n",
        "            return [convert_to_serializable(item) for item in value]\n",
        "        return value\n",
        "\n",
        "    history_data = {\n",
        "        phase: {\n",
        "            metric: convert_to_serializable(values)\n",
        "            for metric, values in metrics.items()\n",
        "        }\n",
        "        for phase, metrics in history.items()\n",
        "    }\n",
        "\n",
        "    with open(history_filename, 'w') as f:\n",
        "        json.dump(history_data, f, indent=4)\n",
        "\n",
        "    # Load checkpoint without weights_only flag\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    # print(\"\\nCheckpoint contents:\")\n",
        "    # for key in checkpoint.keys():\n",
        "    #     print(f\"- {key}\")\n",
        "\n",
        "    # Convert metrics to basic Python types\n",
        "    epoch_data = {\n",
        "        'best_epoch': checkpoint['epoch'],\n",
        "        'best_val_loss': checkpoint['best_val_loss'],\n",
        "        'val_metrics': convert_to_serializable(checkpoint['metrics'])\n",
        "    }\n",
        "\n",
        "    with open(bestepoch_filename, 'w') as f:\n",
        "        json.dump(epoch_data, f, indent=4)\n",
        "\n",
        "    print(f\"\\nSaved training history to: {history_filename}\")\n",
        "    print(f\"Saved best epoch info to: {bestepoch_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2RI6l8bUXwH"
      },
      "source": [
        "## Model Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EayeD9l0u2V",
        "outputId": "8aba5156-53c4-4499-a383-3b5ea7931e69"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "strategy = 'st2'\n",
        "num_cd_classes = len(CLASSES)  #num classes in change mask\n",
        "weighting_method = 'square_balanced'  #balanced,square_balanced,custom\n",
        "\n",
        "weighted_metrics = True if num_cd_classes > 5 else False   #True for 13 classes,False for 3 classes\n",
        "\n",
        "name = f\"{strategy}_{MODEL_NAME}-{num_cd_classes}_classes_{NUM_EPOCHS}\"\n",
        "checkpoint_path = f'{SAVING_DIR}/best_{name}.pt'\n",
        "\n",
        "if MODEL_NAME == 'siamunet_conc':\n",
        "    model = SiamUnet_conc(input_nbr=3, label_nbr=num_cd_classes).to(device)\n",
        "elif MODEL_NAME == 'siamunet_diff':\n",
        "    model = SiamUnet_diff(input_nbr=3, label_nbr=num_cd_classes).to(device)\n",
        "elif MODEL_NAME == 'siamunet_EF':\n",
        "    model = SiamUnet_EF(input_nbr=3, label_nbr=num_cd_classes).to(device)\n",
        "elif MODEL_NAME == 'snunet_conc':\n",
        "    model = Siam_NestedUNet_Conc(in_ch=3, out_ch=num_cd_classes).to(device)\n",
        "elif MODEL_NAME == 'snunet_ECAM':\n",
        "    model = SNUNet_ECAM(in_ch=3, out_ch=num_cd_classes).to(device)\n",
        "\n",
        "model2, history = train_model_balanced(model, train_loader, val_loader,\n",
        "                                      num_epochs=NUM_EPOCHS, num_cd_classes=num_cd_classes,\n",
        "                                      device=device,\n",
        "                                      weighting_method=weighting_method,\n",
        "                                      checkpoint_path=checkpoint_path,\n",
        "                                      weighted_metrics=weighted_metrics)\n",
        "\n",
        "\n",
        "history_filename = f\"{SAVING_DIR}/{name}_history.json\"\n",
        "bestepoch_filename = f\"{SAVING_DIR}/{name}_best_epoch.json\"\n",
        "save_training_files(history=history,checkpoint_path=checkpoint_path,\n",
        "                    history_filename=history_filename,bestepoch_filename=bestepoch_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa0RAqCdChmI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "id": "i6ngPdKERbCE",
        "outputId": "b69c0e9a-36ea-4cf8-b4f0-2f194b4e9093"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "def test_model(model, test_loader, device='cuda',\n",
        "               num_cd_classes=3, weighting_method='square_balanced',\n",
        "               weighted_metrics=False):\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights, _ = calculate_effective_weights(test_loader, device,\n",
        "                                                   num_cd_classes=num_cd_classes,\n",
        "                                                   method=weighting_method)\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "    # Select loss function\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "    # For visualization and metrics\n",
        "    random_samples = []\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Collect predictions and labels for comprehensive metrics\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs1, inputs2, labels in test_loader:\n",
        "            inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs1, inputs2)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item() * inputs1.size(0)\n",
        "            total_samples += inputs1.size(0)\n",
        "\n",
        "            # Get predictions\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Store predictions and labels\n",
        "            all_predictions.append(preds.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "            # Store random samples for visualization\n",
        "            if len(random_samples) < 5:\n",
        "                for i in range(min(inputs1.size(0), 5 - len(random_samples))):\n",
        "                    if random.random() < 0.2:  # 20% chance to select each sample\n",
        "                        random_samples.append({\n",
        "                            'image1': inputs1[i].cpu(),\n",
        "                            'image2': inputs2[i].cpu(),\n",
        "                            'label': labels[i].cpu(),\n",
        "                            'pred': preds[i].cpu(),\n",
        "                            'probabilities': torch.softmax(outputs[i], dim=0).cpu()\n",
        "                        })\n",
        "\n",
        "    # Concatenate predictions and labels\n",
        "    all_predictions = np.concatenate(all_predictions)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_metrics = calculate_metrics(all_predictions, all_labels,\n",
        "                                     num_cd_classes,\n",
        "                                     weighted_metrics=weighted_metrics)\n",
        "\n",
        "    # Add loss to metrics\n",
        "    test_metrics['loss'] = total_loss / total_samples\n",
        "\n",
        "    # Make sure we have exactly 5 samples\n",
        "    while len(random_samples) < 5:\n",
        "        random_samples.append(random_samples[-1] if random_samples else {\n",
        "            'image1': torch.zeros(3, 64, 64),\n",
        "            'image2': torch.zeros(3, 64, 64),\n",
        "            'label': torch.zeros(64, 64),\n",
        "            'pred': torch.zeros(64, 64),\n",
        "            'probabilities': torch.zeros(3, 64, 64)\n",
        "        })\n",
        "\n",
        "    return random_samples, test_metrics\n",
        "\n",
        "def visualize_results(random_samples, num_cd_classes=3):\n",
        "    # Create a figure with subplots\n",
        "    fig, axes = plt.subplots(5, 4, figsize=(25, 25))\n",
        "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "    for idx, sample in enumerate(random_samples):\n",
        "        # Normalize and convert images for display\n",
        "        img1 = sample['image1'].numpy().transpose(1, 2, 0)\n",
        "        img2 = sample['image2'].numpy().transpose(1, 2, 0)\n",
        "        img1 = (img1 - img1.min()) / (img1.max() - img1.min())\n",
        "        img2 = (img2 - img2.min()) / (img2.max() - img2.min())\n",
        "\n",
        "        # Get masks\n",
        "        pred_mask = sample['pred'].numpy()\n",
        "        true_mask = sample['label'].numpy()\n",
        "\n",
        "        # Plot images and masks\n",
        "        axes[idx, 0].imshow(img1)\n",
        "        axes[idx, 0].set_title('Image 1')\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        axes[idx, 1].imshow(img2)\n",
        "        axes[idx, 1].set_title('Image 2')\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Plot predicted mask\n",
        "        axes[idx, 2].imshow(pred_mask, cmap='tab10', vmin=0, vmax=num_cd_classes-1)\n",
        "        axes[idx, 2].set_title('Predicted Change')\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "        # Plot ground truth mask\n",
        "        axes[idx, 3].imshow(true_mask, cmap='tab10', vmin=0, vmax=num_cd_classes-1)\n",
        "        axes[idx, 3].set_title('Ground Truth')\n",
        "        axes[idx, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_test_metrics(test_metrics, save_path):\n",
        "    \"\"\"Save test metrics to JSON\"\"\"\n",
        "    # Use the pre-computed metrics directly\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(test_metrics, f, indent=4)\n",
        "\n",
        "    print(f\"\\nSaved test metrics to: {save_path}\")\n",
        "\n",
        "# Test the model\n",
        "random_samples, test_metrics = test_model(model, test_loader, device=device,\n",
        "                                          num_cd_classes=num_cd_classes,\n",
        "                                          weighted_metrics=weighted_metrics)\n",
        "\n",
        "# Save test metrics\n",
        "save_test_metrics(test_metrics=test_metrics,\n",
        "                  save_path=f'{SAVING_DIR}/{name}_test_metrics.json')\n",
        "\n",
        "# Visualize results\n",
        "visualize_results(random_samples,num_cd_classes=num_cd_classes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5939633,
          "sourceId": 9710583,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
