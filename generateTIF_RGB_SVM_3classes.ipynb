{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code to Generate TIF from Google Earth Engine\n",
        "### Requirements: \n",
        "- Google Earth Engine Project\n",
        "- Google Drive access (to store the accuracy metrics in a csv)\n",
        "- Google drive access (to store the generated TIFs)\n",
        "- A `coordinates.xlsx` file - to copy the coordinates from \n",
        "\n",
        "### Number of classes - 3\n",
        "`{Water, Building, Vegetation}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iVsWd1M7VIS"
      },
      "source": [
        "### Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yJoUC6vZ6Rar"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "ee.Initialize(project='omkarsoak-ee')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm6FJlGjYcdX",
        "outputId": "a262d6a5-b435-4987-9e27-54eefa505157"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "csv_filename = '/content/drive/My Drive/BTechProject/classification_results_veg.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4q8Elxgt69h"
      },
      "source": [
        "### COMBINED RGB + MASK SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ie-0yo0qt2L7"
      },
      "outputs": [],
      "source": [
        "def process_point_svm(lon, lat, city, state, year, filename):\n",
        "    # Define the center point\n",
        "    center = ee.Geometry.Point([lon, lat])\n",
        "\n",
        "    # Define the bounding box with a buffer radius (2560 meters for 512x512 pixels at 10m resolution)\n",
        "    geometry = center.buffer(2560).bounds()\n",
        "\n",
        "    # Load Sentinel-2 MSI Level-2A as ImageCollection\n",
        "    sentinel2 = (ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
        "                 .filterBounds(geometry)\n",
        "                 .filterDate(f'{year}-07-01', f'{year}-09-30')\n",
        "                 .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 3))\n",
        "                 .median())\n",
        "\n",
        "    # Calculate indices and masks\n",
        "    ndvi = sentinel2.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "    swm = sentinel2.expression(\n",
        "        '(B2 + B3) / (B8 + B11)',\n",
        "        {\n",
        "            'B2': sentinel2.select('B2'),\n",
        "            'B3': sentinel2.select('B3'),\n",
        "            'B8': sentinel2.select('B8'),\n",
        "            'B11': sentinel2.select('B11')\n",
        "        }\n",
        "    ).rename('SWM')\n",
        "\n",
        "    # Combine NDVI and SWM into a single image for sampling\n",
        "    feature_image = ndvi.addBands(swm)\n",
        "\n",
        "    # Define class masks\n",
        "    water_mask = swm.gt(1.5)\n",
        "    building_mask = ndvi.gt(0.0).And(ndvi.lt(0.4))\n",
        "    veg_mask = ndvi.gte(0.4)\n",
        "\n",
        "    # Sample points for each class\n",
        "    NUM_OF_PIXELS = 70000\n",
        "\n",
        "    def sample_points(mask, class_value):\n",
        "        return feature_image.updateMask(mask).sample(\n",
        "            region=geometry,\n",
        "            scale=10,\n",
        "            numPixels=NUM_OF_PIXELS,\n",
        "            geometries=True\n",
        "        ).map(lambda f: f.set('class', class_value))\n",
        "\n",
        "    water_points = sample_points(water_mask, 0)\n",
        "    building_points = sample_points(building_mask, 1)\n",
        "    veg_points = sample_points(veg_mask, 2)\n",
        "\n",
        "    # Merge all training points\n",
        "    training_points = water_points.merge(building_points).merge(veg_points)\n",
        "\n",
        "    # Split the data into training (80%) and validation (20%) sets\n",
        "    with_random = training_points.randomColumn()\n",
        "    split = 0.8\n",
        "    training_data = with_random.filter(ee.Filter.lt('random', split))\n",
        "    validation_data = with_random.filter(ee.Filter.gte('random', split))\n",
        "\n",
        "    # Train an SVM classifier\n",
        "    svm_classifier = ee.Classifier.libsvm().train(\n",
        "        features=training_data,\n",
        "        classProperty='class',\n",
        "        inputProperties=['NDVI', 'SWM']\n",
        "    )\n",
        "\n",
        "    # Classify the image\n",
        "    classified_image = feature_image.classify(svm_classifier)\n",
        "    # classified_image = classified_image.toUint16()\n",
        "\n",
        "    # # Evaluate the classifier using the validation data\n",
        "    # validation = validation_data.classify(svm_classifier)\n",
        "    # validation_confusion_matrix = validation.errorMatrix('class', 'classification')\n",
        "    # print('Validation Error Matrix:', validation_confusion_matrix.getInfo())\n",
        "    # print('Validation Overall Accuracy:', validation_confusion_matrix.accuracy().getInfo())\n",
        "\n",
        "    # # Compute training accuracy\n",
        "    # training_confusion_matrix = svm_classifier.confusionMatrix()\n",
        "    # print('Training Error Matrix:', training_confusion_matrix.getInfo())\n",
        "    # print('Training Overall Accuracy:', training_confusion_matrix.accuracy().getInfo())\n",
        "\n",
        "\n",
        "    # Evaluate the classifier using the validation data\n",
        "    validation = validation_data.classify(svm_classifier)\n",
        "    validation_confusion_matrix = validation.errorMatrix('class', 'classification')\n",
        "    validation_accuracy = validation_confusion_matrix.accuracy().getInfo()\n",
        "\n",
        "    # Compute training accuracy\n",
        "    training_confusion_matrix = svm_classifier.confusionMatrix()\n",
        "    training_accuracy = training_confusion_matrix.accuracy().getInfo()\n",
        "\n",
        "    # # Save results to CSV\n",
        "    # csv_filename = \"classification_results.csv\"\n",
        "    header = [\"City\", \"State\", \"Year\", \"Filename\",\n",
        "              \"Training Accuracy\", \"Training Confusion Matrix\",\n",
        "              \"Validation Accuracy\", \"Validation Confusion Matrix\"]\n",
        "\n",
        "    data = [\n",
        "        city, state, year, filename,\n",
        "        training_accuracy, training_confusion_matrix.getInfo(),\n",
        "        validation_accuracy, validation_confusion_matrix.getInfo()\n",
        "    ]\n",
        "\n",
        "    # Write to CSV (append if file exists, otherwise create)\n",
        "    try:\n",
        "        with open(csv_filename, 'a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            if file.tell() == 0:  # File is empty, write header\n",
        "                writer.writerow(header)\n",
        "            writer.writerow(data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to CSV: {e}\")\n",
        "\n",
        "    # print(f\"Results saved to {csv_filename}.\")\n",
        "\n",
        "    # try:\n",
        "    #   file_exists = os.path.isfile(csv_filename)\n",
        "    #   with open(csv_filename, 'a', newline='') as file:\n",
        "    #       writer = csv.writer(file)\n",
        "    #       if not file_exists:  # If the file doesn't exist, write the header\n",
        "    #           writer.writerow(header)\n",
        "    #       writer.writerow(data)\n",
        "    # except Exception as e:\n",
        "    #   print(f\"Error writing to CSV: {e}\")\n",
        "\n",
        "    print(f\"Results saved to {csv_filename}.\")\n",
        "\n",
        "    # Enhanced RGB export\n",
        "    enhanced_rgb = sentinel2.select(['B4', 'B3', 'B2']).multiply(2.0)\n",
        "    viz_params = {\n",
        "        'bands': ['B4', 'B3', 'B2'],\n",
        "        'min': 0,\n",
        "        'max': 3000,\n",
        "        'gamma': 1\n",
        "    }\n",
        "\n",
        "    # Export tasks\n",
        "    # 1. Enhanced RGB\n",
        "    enhanced_rgb_task = ee.batch.Export.image.toDrive(\n",
        "        image=enhanced_rgb.uint16().clip(geometry).visualize(**viz_params),\n",
        "        description=f'EnhancedRGB_{filename}_{year}',\n",
        "        folder='ChangeDetectionVeg',\n",
        "        fileNamePrefix=f'{filename}_{year}',\n",
        "        region=geometry,\n",
        "        fileFormat='GEOTIFF',\n",
        "        crs='EPSG:3857',\n",
        "        dimensions='512x512'\n",
        "    )\n",
        "\n",
        "    # 2. Classified Image\n",
        "    class_vis = {\n",
        "        'min': 0,\n",
        "        'max': 2,\n",
        "        'palette': ['lightblue', 'white', 'lightgreen']\n",
        "    }\n",
        "    mask_task = ee.batch.Export.image.toDrive(\n",
        "        image=classified_image.uint16(),\n",
        "        description=f'm_{filename}_{year}',\n",
        "        folder='ChangeDetectionVeg',\n",
        "        fileNamePrefix=f'm_{filename}_{year}',\n",
        "        region=geometry,\n",
        "        fileFormat='GEOTIFF',\n",
        "        crs='EPSG:3857',\n",
        "        dimensions='512x512'\n",
        "    )\n",
        "\n",
        "    # Start both export tasks\n",
        "    enhanced_rgb_task.start()\n",
        "    mask_task.start()\n",
        "\n",
        "    print(f\"Tasks started for {city} at ({lon}, {lat}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import data from csv\n",
        "- directly copy paste into a cell\n",
        "- just do `data = pd.read_csv(...`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hpibLWhi_0an"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "data = pd.read_csv(io.StringIO('''\n",
        "Oklahoma,Tulsa,2019,2024,\"-95.9995467029476,36.1528559311182\",36.15285593,-95.9995467,\"-95.9995467029476,36.2028559311182\",\"-95.9495467029476,36.2028559311182\",\"-95.9495467029476,36.1528559311182\",\"-95.9495467029476,36.1028559311182\",\"-95.9995467029476,36.1028559311182\",\"-96.0495467029476,36.1028559311182\",\"-96.0495467029476,36.1528559311182\",\"-96.0495467029476,36.2028559311182\"\n",
        "Alabama,Birmingham,2019,2024,\"-86.7578442477576,33.5216943233523\",33.52169432,-86.75784425,\"-86.7578442477576,33.5716943233523\",\"-86.7078442477576,33.5716943233523\",\"-86.7078442477576,33.5216943233523\",\"-86.7078442477576,33.4716943233523\",\"-86.7578442477576,33.4716943233523\",\"-86.8078442477576,33.4716943233523\",\"-86.8078442477576,33.5216943233523\",\"-86.8078442477576,33.5716943233523\"\n",
        "'''), header=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO8a-E6GQpOS"
      },
      "source": [
        "### RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "bCaj8N-DKj4c",
        "outputId": "64e57106-c2bc-43ce-d8a7-afcbfc8cb322"
      },
      "outputs": [],
      "source": [
        "coord_list = [4, 7, 8, 9, 10, 11, 12, 13, 14]\n",
        "coord_name = ['C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n",
        "ncities = data.shape[0]\n",
        "\n",
        "# Process each city\n",
        "for j in range(ncities):\n",
        "    cityData = data.iloc[j]\n",
        "    city = cityData[1]\n",
        "    state = cityData[0]\n",
        "    print(city, state)\n",
        "\n",
        "    for i in range(9):\n",
        "        try:\n",
        "            coord = cityData.iloc[coord_list[i]].split(',')\n",
        "            filename = f'{state}_{city}_{coord_name[i]}'\n",
        "            print(filename)\n",
        "\n",
        "            lon = float(coord[0])\n",
        "            lat = float(coord[1])\n",
        "            process_point_svm(lon, lat, city, state, '2019', filename)\n",
        "            process_point_svm(lon, lat, city, state, '2024', filename)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {state}, {city}, {coord_name[i]}: {e}\")\n",
        "            print(f\"Exception details: {type(e)._name_} - {e}\")\n",
        "    print()\n",
        "\n",
        "print(\"All tasks have been started.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run (without error handling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coord_list = [4, 7, 8, 9, 10, 11, 12, 13, 14]\n",
        "coord_name = ['C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n",
        "ncities = data.shape[0]\n",
        "# Process each city\n",
        "for j in range(ncities):\n",
        "  cityData = data.iloc[j]\n",
        "  city = cityData[1]\n",
        "  state = cityData[0]\n",
        "  # print(cityData)\n",
        "  print(city, state)\n",
        "  for i in range(9):\n",
        "      coord = cityData.iloc[coord_list[i]].split(',')\n",
        "      filename = f'{state}_{city}_{coord_name[i]}'\n",
        "      print(filename, end=';')\n",
        "\n",
        "      lon = float(coord[0])\n",
        "      lat = float(coord[1])\n",
        "      print(lon, lat)\n",
        "      # process_point(lon, lat, city, state, '2019', filename)\n",
        "      # process_point(lon, lat, city, state, '2024', filename)\n",
        "      process_point_svm(lon, lat, city, state, '2019', filename)\n",
        "      process_point_svm(lon, lat, city, state, '2024', filename)\n",
        "  print()\n",
        "\n",
        "print(\"All tasks have been started.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
