{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making Model Prediction masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Get the prediction mask for a specific city tile using the `.pt` model file and the `model class`\n",
        "- Load the specific city from the dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMowMJXZChmE"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRKZrxCtChmG",
        "outputId": "30a3cbc0-fd5a-444e-d109-fe87d4fbbc9d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import rasterio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class ChangeDetectionDatasetTIF(Dataset):\n",
        "    def __init__(self, t2019_dir, t2024_dir, mask_dir,classes, transform=None):\n",
        "        self.t2019_dir = t2019_dir\n",
        "        self.t2024_dir = t2024_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.classes = classes  # Change detection classes\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load all paths\n",
        "        self.t2019_paths = sorted([f for f in os.listdir(t2019_dir) if f.endswith('.tif')])\n",
        "        self.t2024_paths = sorted([f for f in os.listdir(t2024_dir) if f.endswith('.tif')])\n",
        "        self.mask_paths = sorted([f for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.t2019_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load images using rasterio\n",
        "        with rasterio.open(os.path.join(self.t2019_dir, self.t2019_paths[index])) as src:\n",
        "            img_t2019 = src.read(out_dtype=np.float32) / 255.0\n",
        "        with rasterio.open(os.path.join(self.t2024_dir, self.t2024_paths[index])) as src:\n",
        "            img_t2024 = src.read(out_dtype=np.float32) / 255.0\n",
        "        # Load masks\n",
        "        with rasterio.open(os.path.join(self.mask_dir, self.mask_paths[index])) as src:\n",
        "            cd_mask = src.read(1).astype(np.int64)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        img_t2019 = torch.from_numpy(img_t2019)\n",
        "        img_t2024 = torch.from_numpy(img_t2024)\n",
        "        cd_mask = torch.from_numpy(cd_mask)\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform is not None:\n",
        "            img_t2019 = self.transform(img_t2019)\n",
        "            img_t2024 = self.transform(img_t2024)\n",
        "\n",
        "        return img_t2019, img_t2024, cd_mask\n",
        "\n",
        "def describe_loader(loader_type):\n",
        "    img2019, img2024, cd_mask = next(iter(loader_type))\n",
        "    print(\"Batch size:\", loader_type.batch_size)\n",
        "    print(\"2019 Image Shape:\", img2019.shape)\n",
        "    print(\"2024 Image Shape:\", img2024.shape)\n",
        "    print(\"Change Mask Shape:\", cd_mask.shape)\n",
        "    print(\"Number of images:\", len(loader_type.dataset))\n",
        "    print(\"Classes:\", loader_type.dataset.classes)\n",
        "    print(\"Unique CD values:\", torch.unique(cd_mask))\n",
        "\n",
        "# Example usage:\n",
        "ROOT_DIRECTORY = \"ChangeDetectionMergedDividedSplit-tif3\"\n",
        "SAVING_DIR = \"image_saver\"\n",
        "CD_DIR = \"cd2_Output\"\n",
        "#CLASSES = ['no_change','vegetation_increase','vegetation_decrease']\n",
        "CLASSES = ['no_change', 'water_building', 'water_sparse', 'water_dense',\n",
        "           'building_water', 'building_sparse', 'building_dense',\n",
        "           'sparse_water', 'sparse_building', 'sparse_dense',\n",
        "           'dense_water', 'dense_building', 'dense_sparse']\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/train/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/train/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/train/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "val_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/val/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/val/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/val/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "test_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/test/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/test/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/test/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "### KEEP SHUFFLE=FALSE (to get same sample index each time)\n",
        "num_workers = 8\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)#,num_workers=num_workers, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)#,num_workers=num_workers, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)#,num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(\"------------Train-----------\")\n",
        "describe_loader(train_loader)\n",
        "print(\"------------Val------------\")\n",
        "describe_loader(val_loader)\n",
        "print(\"------------Test------------\")\n",
        "describe_loader(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKI0wDXtChmI"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g7C2m2w0E3Rb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import segmentation_models_pytorch as smp\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "class Strategy3Model:\n",
        "    \"\"\"Combined CD and LCM model with checkpoint management\"\"\"\n",
        "    def __init__(self, cd_architecture='unet', lcm_architecture='unet',\n",
        "                 cd_encoder='resnet34', lcm_encoder='resnet34',\n",
        "                 input_channels=3, num_classes=13, num_semantic_classes=4):\n",
        "        # Initialize CD model\n",
        "        self.cd_model = self._create_cd_model(\n",
        "            architecture=cd_architecture,\n",
        "            encoder=cd_encoder,\n",
        "            input_channels=input_channels\n",
        "        )\n",
        "        # Initialize LCM model\n",
        "        self.lcm_model = self._create_lcm_model(\n",
        "            architecture=lcm_architecture,\n",
        "            encoder=lcm_encoder,\n",
        "            input_channels=input_channels,\n",
        "            num_semantic_classes=num_semantic_classes\n",
        "        )\n",
        "\n",
        "    def _create_cd_model(self, architecture, encoder, input_channels):\n",
        "        \"\"\"Create binary change detection model\"\"\"\n",
        "        if architecture.lower() == 'unet':\n",
        "            model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights='imagenet',\n",
        "                in_channels=input_channels*2,  # Concatenated images\n",
        "                classes=1,  # Binary output,\n",
        "                encoder_depth=4,  # Reduce depth (def=5)\n",
        "                decoder_channels=(256, 128, 64, 32)  # Reduce channels(def=(256, 128, 64, 32, 16))\n",
        "\n",
        "            )\n",
        "        elif architecture.lower() == 'deeplabv3plus':\n",
        "            model = smp.DeepLabV3Plus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights='imagenet',\n",
        "                in_channels=input_channels*2,\n",
        "                classes=1,\n",
        "            )\n",
        "        # Add more architectures as needed\n",
        "        return model\n",
        "\n",
        "    def _create_lcm_model(self, architecture, encoder, input_channels, num_semantic_classes=4):\n",
        "        \"\"\"Create land cover mapping model\"\"\"\n",
        "        if architecture.lower() == 'unet':\n",
        "            model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights='imagenet',\n",
        "                in_channels=input_channels,\n",
        "                classes=num_semantic_classes,  # 4 land cover classes\n",
        "            )\n",
        "        elif architecture.lower() == 'deeplabv3plus':\n",
        "            model = smp.DeepLabV3Plus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights='imagenet',\n",
        "                in_channels=input_channels,\n",
        "                classes=num_semantic_classes,\n",
        "            )\n",
        "        # Add more architectures as needed\n",
        "        return model\n",
        "\n",
        "    def to(self, device):\n",
        "        \"\"\"Move models to device\"\"\"\n",
        "        self.cd_model = self.cd_model.to(device)\n",
        "        self.lcm_model = self.lcm_model.to(device)\n",
        "        return self\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Set models to training mode\"\"\"\n",
        "        self.cd_model.train()\n",
        "        self.lcm_model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Set models to evaluation mode\"\"\"\n",
        "        self.cd_model.eval()\n",
        "        self.lcm_model.eval()\n",
        "\n",
        "def create_semantic_change_mask(binary_pred, lcm_pred_2019, lcm_pred_2024):\n",
        "    \"\"\"Convert binary change + LCM predictions to 13-class semantic change mask.\n",
        "\n",
        "    Optimized version using vectorized operations and pre-computed lookup tables.\n",
        "\n",
        "    Args:\n",
        "        binary_pred: Binary change prediction tensor (B, 1, H, W)\n",
        "        lcm_pred_2019: Land cover prediction tensor for 2019 (B, C, H, W)\n",
        "        lcm_pred_2024: Land cover prediction tensor for 2024 (B, C, H, W)\n",
        "\n",
        "    Returns:\n",
        "        Semantic change mask tensor (B, H, W) with values 0-12\n",
        "    \"\"\"\n",
        "    device = binary_pred.device\n",
        "    batch_size = binary_pred.shape[0]\n",
        "    height = binary_pred.shape[2]\n",
        "    width = binary_pred.shape[3]\n",
        "\n",
        "    # Pre-compute land cover predictions - do this once\n",
        "    lcm_2019 = torch.argmax(lcm_pred_2019, dim=1)  # (B, H, W)\n",
        "    lcm_2024 = torch.argmax(lcm_pred_2024, dim=1)  # (B, H, W)\n",
        "\n",
        "    # Create the change mask - use threshold without squeeze/unsqueeze\n",
        "    change_mask = binary_pred[:, 0] > 0.5  # (B, H, W)\n",
        "\n",
        "    # Initialize output tensor\n",
        "    semantic_mask = torch.zeros((batch_size, height, width), device=device, dtype=torch.long)\n",
        "\n",
        "    # Create transition matrix lookup table - speeds up class mapping\n",
        "    # Format: from_class * num_classes + to_class = semantic_class\n",
        "    num_classes = 4  # Water, Building, Sparse, Dense\n",
        "    transitions = torch.full((num_classes * num_classes,), 0, device=device)\n",
        "\n",
        "    # Populate transition matrix - all transitions not listed default to 0 (no change)\n",
        "    transition_map = {\n",
        "        (0, 1): 1,   # Water → Building\n",
        "        (0, 2): 2,   # Water → Sparse\n",
        "        (0, 3): 3,   # Water → Dense\n",
        "        (1, 0): 4,   # Building → Water\n",
        "        (1, 2): 5,   # Building → Sparse\n",
        "        (1, 3): 6,   # Building → Dense\n",
        "        (2, 0): 7,   # Sparse → Water\n",
        "        (2, 1): 8,   # Sparse → Building\n",
        "        (2, 3): 9,   # Sparse → Dense\n",
        "        (3, 0): 10,  # Dense → Water\n",
        "        (3, 1): 11,  # Dense → Building\n",
        "        (3, 2): 12,  # Dense → Sparse\n",
        "    }\n",
        "\n",
        "    for (from_idx, to_idx), semantic_idx in transition_map.items():\n",
        "        transitions[from_idx * num_classes + to_idx] = semantic_idx\n",
        "\n",
        "    # Vectorized computation of semantic classes\n",
        "    # Only compute for changed pixels to save memory\n",
        "    changed_pixels = change_mask.nonzero(as_tuple=True)\n",
        "    if len(changed_pixels[0]) > 0:\n",
        "        from_classes = lcm_2019[changed_pixels]  # (N,)\n",
        "        to_classes = lcm_2024[changed_pixels]    # (N,)\n",
        "\n",
        "        # Compute transition indices\n",
        "        transition_indices = from_classes * num_classes + to_classes  # (N,)\n",
        "\n",
        "        # Look up semantic classes from transition matrix\n",
        "        semantic_classes = transitions[transition_indices]  # (N,)\n",
        "\n",
        "        # Assign semantic classes to output mask\n",
        "        semantic_mask[changed_pixels] = semantic_classes\n",
        "\n",
        "    return semantic_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the checkpoint path, model name etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Strategy 1: PCC - Linknet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Initialize model and device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# architecture = 'linknet'  # 'unet' or 'linknet', 'pspnet', 'deeplabv3plus'\n",
        "# num_classes = 13   # Change Detection classes (3 for cd1, 13 for cd2)\n",
        "# num_semantic_classes = 4   # Semantic segmentation LCM classes (4 for both)\n",
        "# num_epochs = 100\n",
        "# loss = 'CE'\n",
        "# checkpoint_path = f'{SAVING_DIR}/best_{architecture}_{num_epochs}_epochs.pt'\n",
        "\n",
        "# # Create model\n",
        "# model = ChangeDetectionModel(\n",
        "#     architecture=architecture,encoder='resnet34',\n",
        "#     input_channels=3,num_classes=num_classes,\n",
        "#     num_semantic_classes=num_semantic_classes\n",
        "# ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Strategy 2: Siam Diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Initialize and train the model\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model_name = 'siamunet_diff'\n",
        "# strategy = 'st2' #change detection strategy {1,2,3,4}\n",
        "# num_classes = 13  #num classes in change mask\n",
        "# num_epochs = 100\n",
        "# weighting_method = 'square_balanced' #'custom'\n",
        "# loss = 'CE' #'focal' #'bcl'\n",
        "# #checkpoint_path = f'{SAVING_DIR}/best_{strategy}_{model_name}_{num_epochs}.pt'\n",
        "# checkpoint_path = f'{SAVING_DIR}/best_{strategy}_{model_name}-{num_classes}_classes_{num_epochs}.pt'\n",
        "\n",
        "# model = SiamUnet_diff(input_nbr=3, label_nbr=num_classes).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Strategy 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes=13   #Change Detection classes (3 for cd1, 13 for cd2)\n",
        "num_semantic_classes=4  #Semantic Segmentation LCM classes (4 for both)\n",
        "num_epochs = 100\n",
        "weighting_method = 'square_balanced'\n",
        "loss = 'CE' #'CE'\n",
        "checkpoint_path = f'{SAVING_DIR}/best_Strat3_{num_epochs}_epochs.pt'  #'models/strategy3_model.pt'\n",
        "\n",
        "# Create model\n",
        "model = Strategy3Model(\n",
        "    cd_architecture='unet',\n",
        "    lcm_architecture='unet',\n",
        "    cd_encoder='resnet34',\n",
        "    lcm_encoder='resnet34',\n",
        "    input_channels=3,\n",
        "    num_classes=num_classes,\n",
        "    num_semantic_classes=num_semantic_classes\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Strategy 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # Model configuration\n",
        "# input_channels = 3\n",
        "# num_semantic_classes = 4 #len(SEMANTIC_CLASSES)\n",
        "# num_change_classes = 13 #len(CLASSES)\n",
        "# num_classes = num_change_classes\n",
        "# num_epochs = 100\n",
        "\n",
        "# # Create model\n",
        "# model = MultiTaskChangeDetectionModel(\n",
        "#     input_channels=input_channels,\n",
        "#     num_semantic_classes=num_semantic_classes,\n",
        "#     num_change_classes=num_change_classes\n",
        "# ).to(device)\n",
        "\n",
        "# # Define checkpoint paths\n",
        "# lcm_checkpoint_path = f'{SAVING_DIR}/best_lcm_model_{num_epochs}.pt'\n",
        "# full_checkpoint_path = f'{SAVING_DIR}/best_full_model_{num_epochs}.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa0RAqCdChmI"
      },
      "source": [
        "### Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def find_sample_idx_by_filename(test_loader, filename):\n",
        "    \"\"\"\n",
        "    Find the sample index for a given filename in the test_loader dataset.\n",
        "    \n",
        "    Args:\n",
        "        test_loader: DataLoader containing the test data\n",
        "        filename: Filename to search for (without path)\n",
        "    \n",
        "    Returns:\n",
        "        int: Index of the sample, or -1 if not found\n",
        "    \"\"\"\n",
        "    # Access the underlying dataset\n",
        "    dataset = test_loader.dataset\n",
        "    \n",
        "    # Search through all paths in the dataset\n",
        "    for idx in range(len(dataset)):\n",
        "        # Get the t2019 paths from your dataset (adjust this based on your dataset structure)\n",
        "        current_path = dataset.t2019_paths[idx]\n",
        "        current_filename = os.path.basename(current_path)\n",
        "        \n",
        "        # Check if this is the file we're looking for\n",
        "        if filename in current_filename:\n",
        "            return idx\n",
        "    \n",
        "    return -1\n",
        "\n",
        "def visualize_single_sample(model, test_loader, sample_idx, device='cpu', \n",
        "                          num_classes=3, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize prediction for a specific sample from the test_loader.\n",
        "    \n",
        "    Args:\n",
        "        model: The model to use for prediction\n",
        "        test_loader: DataLoader containing the test data\n",
        "        sample_idx: Index of the sample to visualize\n",
        "        device: Device to run model on\n",
        "        num_classes: Number of classes in change detection\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    ########### STRATEGY 1,2 ################\n",
        "    # checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "    \n",
        "    ########### STRATEGY 3 ################\n",
        "    # checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    # print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    # model.cd_model.load_state_dict(checkpoint['cd_model'])\n",
        "    # model.lcm_model.load_state_dict(checkpoint['lcm_model'])\n",
        "    # print(\"Successfully loaded both CD and LCM models\")\n",
        "\n",
        "    ########### STRATEGY 4 ################\n",
        "    # checkpoint_path=full_checkpoint_path\n",
        "    # checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Get the specific sample\n",
        "    total_batches = len(test_loader)\n",
        "    batch_size = test_loader.batch_size\n",
        "    batch_idx = sample_idx // batch_size\n",
        "    item_idx = sample_idx % batch_size\n",
        "\n",
        "    if batch_idx >= total_batches:\n",
        "        print(f\"Sample index {sample_idx} is out of range. Maximum index is {total_batches * batch_size - 1}\")\n",
        "        return\n",
        "\n",
        "    # Get the specific batch\n",
        "    for i, (inputs1, inputs2, labels) in enumerate(test_loader):\n",
        "        if i == batch_idx:\n",
        "            break\n",
        "\n",
        "    # Extract the specific item from the batch\n",
        "    img1 = inputs1[item_idx]\n",
        "    img2 = inputs2[item_idx]\n",
        "    true_mask = labels[item_idx]\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        # Add batch dimension for model\n",
        "        img1_batch = img1.unsqueeze(0).to(device)\n",
        "        img2_batch = img2.unsqueeze(0).to(device)\n",
        "\n",
        "        ########### STRATEGY 1,2 ################\n",
        "        #output = model(img1_batch, img2_batch)\n",
        "        #pred_mask = torch.argmax(output, dim=1)[0].cpu().numpy()\n",
        "\n",
        "        ########### STRATEGY 4 ################\n",
        "        #_,_,output = model(img1_batch, img2_batch)\n",
        "        #pred_mask = torch.argmax(output, dim=1)[0].cpu().numpy()\n",
        "\n",
        "        ########### STRATEGY 3 ################\n",
        "        cd_pred = model.cd_model(torch.cat([img1.unsqueeze(0), img2.unsqueeze(0)], dim=1))\n",
        "        lcm_pred_2019 = model.lcm_model(img1.unsqueeze(0))\n",
        "        lcm_pred_2024 = model.lcm_model(img2.unsqueeze(0))\n",
        "        semantic_pred = create_semantic_change_mask(cd_pred, lcm_pred_2019, lcm_pred_2024)\n",
        "        pred_mask = semantic_pred.squeeze(0)\n",
        "        \n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    plt.subplots_adjust(wspace=0.3)\n",
        "\n",
        "    # Display original images\n",
        "    img1_display = img1.numpy().transpose(1, 2, 0)\n",
        "    img2_display = img2.numpy().transpose(1, 2, 0)\n",
        "\n",
        "    axes[0].imshow(img1_display)\n",
        "    axes[0].set_title('Image 1')\n",
        "    axes[0].axis('off')\n",
        "    plt.imsave(f\"{save_path}_img2019.png\", img1_display)\n",
        "\n",
        "    axes[1].imshow(img2_display)\n",
        "    axes[1].set_title('Image 2')\n",
        "    axes[1].axis('off')\n",
        "    plt.imsave(f\"{save_path}_img2024.png\", img2_display)\n",
        "    \n",
        "    # 3 classes\n",
        "    #colors = ['black', 'green', 'red']\n",
        "    \n",
        "    # 13 classes\n",
        "    colors = ['black','lightgray', 'gray', 'darkgray',\n",
        "              'darkblue', 'green', 'darkgreen',\n",
        "              'lightblue', 'orange', 'lightgreen',\n",
        "              'blue', 'orangered', 'peachpuff']\n",
        "\n",
        "    cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
        "\n",
        "    # Plot predicted and true masks\n",
        "    axes[2].imshow(pred_mask, cmap=cmap, vmin=0, vmax=num_classes-1)\n",
        "    axes[2].set_title('Predicted Change')\n",
        "    axes[2].axis('off')\n",
        "    plt.imsave(f\"{save_path}_CDprediction.png\", pred_mask, cmap=cmap, vmin=0, vmax=num_classes-1)\n",
        "\n",
        "    axes[3].imshow(true_mask, cmap=cmap, vmin=0, vmax=num_classes-1)\n",
        "    axes[3].set_title('Ground Truth')\n",
        "    axes[3].axis('off')\n",
        "    plt.imsave(f\"{save_path}_CDtruth.png\", true_mask, cmap=cmap, vmin=0, vmax=num_classes-1)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Find sample by filename and visualize\n",
        "filename = 'NorthCarolina_Charlotte_W_2019_2.tif'\n",
        "editedfilename = filename.replace('.tif','')\n",
        "display_loader = test_loader  # Using image from test_loader for visualization\n",
        "model_name = 'strategy4'\n",
        "rooting_dir = f\"{SAVING_DIR}/{model_name}_{num_classes}-classes_{editedfilename}\"\n",
        "\n",
        "if not os.path.exists(rooting_dir):\n",
        "    os.mkdir(rooting_dir)\n",
        "\n",
        "sample_idx = find_sample_idx_by_filename(display_loader, filename)\n",
        "\n",
        "if sample_idx >= 0:\n",
        "    visualize_single_sample(model, display_loader, sample_idx, \n",
        "                            device=device, num_classes=num_classes,\n",
        "                            save_path=f\"{rooting_dir}/{model_name}_{num_classes}-classes_{editedfilename}_\")\n",
        "else:\n",
        "    print(f\"File {filename} not found in dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding good city tiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Image loader\n",
        "- Used for finding good images (for paper)\n",
        "- The `sample_index` is written in image title (used to find the name of the city)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set up the plot size and remove axes\n",
        "fig, axs = plt.subplots(4, 3, figsize=(8,8))\n",
        "\n",
        "for i in range(2):\n",
        "    j = random.randint(0, len(test_dataset) - 1)\n",
        "    # `j` is the sample_index (can be used further)\n",
        "    image1, image2, mask = test_dataset[j]\n",
        "    # Display images\n",
        "    axs[i, 0].imshow(image1.permute(1, 2, 0))\n",
        "    axs[i, 0].set_title(f\"Real 2019 {j}\")\n",
        "    axs[i, 0].axis(\"off\")\n",
        "\n",
        "    axs[i, 1].imshow(image2.permute(1, 2, 0))\n",
        "    axs[i, 1].set_title(f\"Real 2024\")\n",
        "    axs[i, 1].axis(\"off\")\n",
        "\n",
        "    axs[i, 2].imshow(mask, cmap=\"turbo\")\n",
        "    axs[i, 2].set_title(f\"CD Mask\")\n",
        "    axs[i, 2].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrive filename from `index`\n",
        "- use the sample index from above to get the filename from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "loader = 'test'\n",
        "image_dir = f\"ChangeDetectionMergedDividedSplit-tif3/{loader}/Images/T2019\"\n",
        "\n",
        "# Get sorted list of filenames\n",
        "image_filenames = sorted(os.listdir(image_dir))\n",
        "\n",
        "# Define the index\n",
        "idx = 490 # Change to your required index\n",
        "\n",
        "# Retrieve filename using index\n",
        "filename = image_filenames[idx]\n",
        "print(f\"Filename at index {idx}: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating images, masks for paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert entire TIF folder to PNG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import rasterio\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def tif_to_png(tif_path, png_path):\n",
        "    # Read the .tif file using rasterio\n",
        "    with rasterio.open(tif_path) as src:\n",
        "        # Read the image data into a NumPy array\n",
        "        array = src.read()\n",
        "        if array.shape[0] == 3:  # RGB image\n",
        "            array = np.moveaxis(array, 0, -1)  # Reorder dimensions to (H, W, C)\n",
        "        elif array.shape[0] == 1:  # Grayscale image\n",
        "            array = array[0]  # Remove the single-band dimension\n",
        "    \n",
        "    # Normalize the array to range [0, 255] for saving as PNG\n",
        "    array = array - array.min()\n",
        "    array = (array / array.max() * 255).astype(np.uint8)\n",
        "    \n",
        "    # Save the NumPy array as a .png image using Pillow\n",
        "    img = Image.fromarray(array)\n",
        "    img.save(png_path)\n",
        "    print(f\"Saved {png_path}\")\n",
        "\n",
        "def convert_all_tifs_in_folder(input_folder, output_folder):\n",
        "    # Ensure the output folder exists\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    \n",
        "    # List all .tif files in the input folder\n",
        "    tif_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
        "    \n",
        "    if not tif_files:\n",
        "        print(f\"No .tif files found in {input_folder}\")\n",
        "        return\n",
        "\n",
        "    # Convert each .tif file to .png and save in the output folder\n",
        "    for tif_file in tif_files:\n",
        "        tif_path = os.path.join(input_folder, tif_file)\n",
        "        png_path = os.path.join(output_folder, tif_file.replace(\".tif\", \".png\"))\n",
        "        tif_to_png(tif_path, png_path)\n",
        "\n",
        "# Example usage:\n",
        "directory1 = \"./Organized/SouthDakota_SiouxFalls_E\"\n",
        "directory2 = \"./Organized/SouthDakota_SiouxFalls_E\"\n",
        "convert_all_tifs_in_folder(directory1, directory2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display and save the image\n",
        "- Directly from `TIF` to `PNG`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "\n",
        "# Load the .tif image\n",
        "tif_path = \"Organized\\SouthDakota_SiouxFalls_E\\cd2_m_SouthDakota_SiouxFalls_E.tif\"\n",
        "\n",
        "with rasterio.open(tif_path) as src:\n",
        "    image = src.read(1)  # Read the first band (for grayscale images)\n",
        "\n",
        "#colors = ['lightblue', 'white', 'lightgreen','darkgreen']  # Seg mask (all 4 classes present)\n",
        "#colors = ['white', 'lightgreen','darkgreen']  # Seg mask (3 classes)\n",
        "#colors = ['black', 'green','red']  # cd1 - MCD mask (3 classes)\n",
        "colors = ['black','lightgray', 'gray', 'darkgray',   # cd2 - SCD mask (13 classes)\n",
        "              'darkblue', 'green', 'darkgreen',\n",
        "              'lightblue', 'orange', 'lightgreen',\n",
        "              'blue', 'orangered', 'peachpuff']\n",
        "cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
        "\n",
        "# Plot the image with a user-defined colormap\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(image, cmap=cmap)\n",
        "\n",
        "######## Adding a legend ########\n",
        "#import matplotlib.patches as mpatches   \n",
        "# Define the class labels\n",
        "# class_labels = ['0: no change', '1: water to building', '2: water to sparse', '3: water to dense',\n",
        "#                 '4: building to water', '5: building to sparse', '6: building to dense',\n",
        "#                 '7: sparse to water', '8: sparse to building', '9: sparse to dense',\n",
        "#                 '10: dense to water', '11: dense to building', '12: dense to sparse']\n",
        "\n",
        "# Create a list of patches to be shown in the legend\n",
        "#patches = [mpatches.Patch(color=colors[i], label=class_labels[i]) for i in range(len(class_labels))]\n",
        "\n",
        "# Add the legend to the plot\n",
        "#plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), ncol=4,loc='upper left', borderaxespad=0., prop={'family': 'Times New Roman'})\n",
        "#fig = plt.gcf()\n",
        "#fig.subplots_adjust(right=0.8)\n",
        "#fig.set_size_inches(1, 10)\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.imsave(tif_path.replace('.tif', '_colormap.png'), image, cmap=cmap)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting State vs Num Cities graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List all filenames in given folder\n",
        "- useful for counting cities "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_filenames(folder_path):\n",
        "    \"\"\"Returns a list of filenames in the given folder.\"\"\"\n",
        "    return [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "# Example usage\n",
        "folder_path = \"./new_eur\"  # Change this to your folder path\n",
        "filenames = get_filenames(folder_path)\n",
        "#print(type(filenames))\n",
        "\n",
        "\n",
        "filenames2 = list(filenames)\n",
        "filenames3 = []\n",
        "for file in filenames2: \n",
        "    # file = os.path.splitext(os.path.basename(root_directory+file1))[0]\n",
        "    # print(type(file))\n",
        "    file  = file.replace(\"cd1_m_\",\"\")\n",
        "    file = file.replace(\".tif\", \"\")\n",
        "    # file = file.replace(\"_1\",\"\")\n",
        "    # file = file.replace(\"_2\",\"\")\n",
        "    # file = file.replace(\"_3\",\"\")\n",
        "    # file = file.replace(\"_4\",\"\")\n",
        "    file = file[:-2]\n",
        "    if(file[-1]==\"_\"):\n",
        "        file = file[:-1]\n",
        "    filenames3.append(file)\n",
        "    # print(file)\n",
        "\n",
        "filenames4 = list(set(filenames3))\n",
        "filenames4.sort()\n",
        "\n",
        "print(len(filenames4))\n",
        "# for party in filenames4:\n",
        "#     print(party)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replace some filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "replacements = {\n",
        "    'Alabama_Dayton': 'Ohio_Dayton',\n",
        "    'Alabama_Cincinnati': 'Ohio_Cincinnati',\n",
        "    'Alabama_Toledo': 'Ohio_Toledo',\n",
        "    'Illinois_Greenbay': 'Wisconsin_Greenbay',\n",
        "    'Illinois_FortWayne': 'Indiana_FortWayne',\n",
        "    'Illinois_SouthBend': 'Indiana_SouthBend'\n",
        "}\n",
        "\n",
        "# Apply replacements\n",
        "filenames4_replaced = [replacements.get(name, name) for name in filenames4]\n",
        "\n",
        "#print(filenames4_replaced)\n",
        "\n",
        "filenames4_replaced.sort()\n",
        "# for party in filenames4_replaced:\n",
        "#     if party == \"Illinois_Greenbay\":\n",
        "#         print(party)\n",
        "#         print(\"bla\")\n",
        "\n",
        "print(len(filenames4_replaced))\n",
        "\n",
        "state_city_split = [name.split('_') for name in filenames4_replaced]\n",
        "print(state_city_split)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot bar graph for state vs num of cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the number of cities per state\n",
        "state_counts = Counter([state for state, city in state_city_split])\n",
        "\n",
        "# Extract states and their corresponding counts\n",
        "states = list(state_counts.keys())\n",
        "city_counts = list(state_counts.values())\n",
        "\n",
        "print(len(states))\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(15, 7))\n",
        "bars = plt.bar(states, city_counts, color='#9198db')\n",
        "plt.xlabel('Countries')\n",
        "plt.ylabel('Number of Cities')\n",
        "plt.title('Number of Cities per Country in Europe')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5939633,
          "sourceId": 9710583,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
