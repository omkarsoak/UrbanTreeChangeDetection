{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making Model Prediction masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Get the prediction mask for a specific city tile using the `.pt` model file and the `model class`\n",
        "- Load the specific city from the dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT_DIRECTORY = \"ChangeDetectionMergedDividedSplit-tif3\"\n",
        "SAVING_DIR = \"image_saver\"\n",
        "CD_DIR = \"cd2_Output\"\n",
        "\n",
        "if CD_DIR == \"cd1_Output\":\n",
        "    CLASSES = ['no_change','vegetation_increase','vegetation_decrease']\n",
        "elif CD_DIR == \"cd2_Output\":\n",
        "    CLASSES = ['no_change', 'water_building', 'water_sparse', 'water_dense',\n",
        "               'building_water', 'building_sparse', 'building_dense',\n",
        "               'sparse_water', 'sparse_building', 'sparse_dense',\n",
        "               'dense_water', 'dense_building', 'dense_sparse']\n",
        "\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "#MODEL_NAME = 'snunet_ECAM' \n",
        "#'siamunet_conc','siamunet_diff','siamunet_EF','snunet_conc','snunet_ECAM'\n",
        "\n",
        "# ARCHITECTURE = 'pspnet' # 'unet' or 'linknet', 'pspnet', 'deeplabv3plus'\n",
        "# SEMANTIC_CLASSES = ['water', 'building', 'sparse_vegetation', 'dense_vegetation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMowMJXZChmE"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRKZrxCtChmG",
        "outputId": "30a3cbc0-fd5a-444e-d109-fe87d4fbbc9d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import rasterio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class ChangeDetectionDatasetTIF(Dataset):\n",
        "    def __init__(self, t2019_dir, t2024_dir, mask_dir,classes, transform=None):\n",
        "        self.t2019_dir = t2019_dir\n",
        "        self.t2024_dir = t2024_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.classes = classes  # Change detection classes\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load all paths\n",
        "        self.t2019_paths = sorted([f for f in os.listdir(t2019_dir) if f.endswith('.tif')])\n",
        "        self.t2024_paths = sorted([f for f in os.listdir(t2024_dir) if f.endswith('.tif')])\n",
        "        self.mask_paths = sorted([f for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.t2019_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load images using rasterio\n",
        "        with rasterio.open(os.path.join(self.t2019_dir, self.t2019_paths[index])) as src:\n",
        "            img_t2019 = src.read(out_dtype=np.float32) / 255.0\n",
        "        with rasterio.open(os.path.join(self.t2024_dir, self.t2024_paths[index])) as src:\n",
        "            img_t2024 = src.read(out_dtype=np.float32) / 255.0\n",
        "        # Load masks\n",
        "        with rasterio.open(os.path.join(self.mask_dir, self.mask_paths[index])) as src:\n",
        "            cd_mask = src.read(1).astype(np.int64)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        img_t2019 = torch.from_numpy(img_t2019)\n",
        "        img_t2024 = torch.from_numpy(img_t2024)\n",
        "        cd_mask = torch.from_numpy(cd_mask)\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform is not None:\n",
        "            img_t2019 = self.transform(img_t2019)\n",
        "            img_t2024 = self.transform(img_t2024)\n",
        "\n",
        "        return img_t2019, img_t2024, cd_mask\n",
        "\n",
        "def describe_loader(loader_type):\n",
        "    img2019, img2024, cd_mask = next(iter(loader_type))\n",
        "    print(\"Batch size:\", loader_type.batch_size)\n",
        "    print(\"2019 Image Shape:\", img2019.shape)\n",
        "    print(\"2024 Image Shape:\", img2024.shape)\n",
        "    print(\"Change Mask Shape:\", cd_mask.shape)\n",
        "    print(\"Number of images:\", len(loader_type.dataset))\n",
        "    print(\"Classes:\", loader_type.dataset.classes)\n",
        "    print(\"Unique CD values:\", torch.unique(cd_mask))\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/train/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/train/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/train/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "val_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/val/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/val/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/val/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "test_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/test/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/test/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/test/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "### KEEP SHUFFLE=FALSE (to get same sample index each time)\n",
        "num_workers = 8\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)#,num_workers=num_workers, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)#,num_workers=num_workers, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)#,num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(\"------------Train-----------\")\n",
        "describe_loader(train_loader)\n",
        "print(\"------------Val------------\")\n",
        "describe_loader(val_loader)\n",
        "print(\"------------Test------------\")\n",
        "describe_loader(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKI0wDXtChmI"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ALL Siamunets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7C2m2w0E3Rb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.padding import ReplicationPad2d\n",
        "import torch.optim as optim\n",
        "\n",
        "########## SiamUnet_conc ##########\n",
        "class SiamUnet_conc(nn.Module):\n",
        "    \"\"\"SiamUnet_conc segmentation network for multiclass change detection.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(SiamUnet_conc, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr\n",
        "        self.label_nbr = label_nbr  # Added for clarity\n",
        "\n",
        "        # Encoder Layers\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        # Decoder Layers\n",
        "        self.conv43d = nn.ConvTranspose2d(384, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(192, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(96, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(48, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        # Changed to use label_nbr instead of hardcoded value\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        # Multiclass activation (Softmax instead of LogSoftmax)\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
        "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
        "\n",
        "        ####################################################\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
        "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
        "\n",
        "        ####################################################\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), x43_1, x43_2), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), x33_1, x33_2), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), x22_1, x22_2), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), x12_1, x12_2), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)\n",
        "\n",
        "######### SiamUnet_diff #########\n",
        "class SiamUnet_diff(nn.Module):\n",
        "    \"\"\"SiamUnet_diff segmentation network for multiclass change detection.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(SiamUnet_diff, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr\n",
        "        self.label_nbr = label_nbr\n",
        "\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        # Changed to output label_nbr channels for multiclass\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        # Changed from LogSoftmax to regular Softmax\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
        "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
        "\n",
        "        ####################################################\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
        "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)\n",
        "    \n",
        "######### SiamUnet_EF #########\n",
        "class SiamUnet_EF(nn.Module):\n",
        "    \"\"\"Multiclass UNet segmentation network for change detection.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(SiamUnet_EF, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr * 2  # Multiply by 2 since we concatenate two images\n",
        "        self.label_nbr = label_nbr\n",
        "\n",
        "        # Encoder Layers\n",
        "        self.conv11 = nn.Conv2d(input_nbr * 2, 16, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        # Decoder Layers\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        # Change output layer to match number of classes\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        # Change to Softmax for multiclass\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Initial concatenation of input images (early fusion)\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "\n",
        "        # Stage 1 - Encoder\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n",
        "        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2 - Encoder\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3 - Encoder\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4 - Encoder\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4d - Decoder\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), x43), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d - Decoder\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), x33), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d - Decoder\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), x22), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d - Decoder\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), x12), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)\n",
        "\n",
        "########### SNUNet_conc ###########\n",
        "class conv_block_nested(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        identity = x\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        output = self.activation(x + identity)\n",
        "        return output\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2,\n",
        "                            mode='bilinear',\n",
        "                            align_corners=True) if bilinear else \\\n",
        "                 nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class Siam_NestedUNet_Conc(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3):\n",
        "        super(Siam_NestedUNet_Conc, self).__init__()\n",
        "        torch.nn.Module.dump_patches = True\n",
        "        n1 = 32     # Initial number of channels\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Encoder path for both images\n",
        "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        self.Up1_0 = up(filters[1])\n",
        "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        self.Up2_0 = up(filters[2])\n",
        "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        self.Up3_0 = up(filters[3])\n",
        "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "        self.Up4_0 = up(filters[4])\n",
        "\n",
        "        # Nested dense connections with batch norm\n",
        "        self.conv0_1 = conv_block_nested(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_1 = up(filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_1 = up(filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] * 2 + filters[4], filters[3], filters[3])\n",
        "        self.Up3_1 = up(filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_2 = up(filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2] * 3 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_2 = up(filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1] * 4 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_3 = up(filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0] * 5 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        # Add batch normalization to deep supervision outputs\n",
        "        self.final1 = nn.Sequential(\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.final2 = nn.Sequential(\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.final3 = nn.Sequential(\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.final4 = nn.Sequential(\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        # Final combination layer with better feature extraction\n",
        "        self.conv_final = nn.Sequential(\n",
        "            nn.Conv2d(out_ch * 4, filters[0], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(filters[0] // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0] // 2, out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Use Xavier/Glorot initialization for final layers\n",
        "                if m.kernel_size[0] == 1:\n",
        "                    nn.init.xavier_uniform_(m.weight)\n",
        "                else:\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, xA, xB):\n",
        "        # Encoder Path A\n",
        "        x0_0A = self.conv0_0(xA)\n",
        "        x1_0A = self.conv1_0(self.pool(x0_0A))\n",
        "        x2_0A = self.conv2_0(self.pool(x1_0A))\n",
        "        x3_0A = self.conv3_0(self.pool(x2_0A))\n",
        "\n",
        "        # Encoder Path B\n",
        "        x0_0B = self.conv0_0(xB)\n",
        "        x1_0B = self.conv1_0(self.pool(x0_0B))\n",
        "        x2_0B = self.conv2_0(self.pool(x1_0B))\n",
        "        x3_0B = self.conv3_0(self.pool(x2_0B))\n",
        "        x4_0B = self.conv4_0(self.pool(x3_0B))\n",
        "\n",
        "        # Nested Dense Connections and Decoder Path\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n",
        "\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n",
        "\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n",
        "\n",
        "        # Get outputs at different scales\n",
        "        output1 = self.final1(x0_1)\n",
        "        output2 = self.final2(x0_2)\n",
        "        output3 = self.final3(x0_3)\n",
        "        output4 = self.final4(x0_4)\n",
        "\n",
        "        # Combine outputs\n",
        "        output = self.conv_final(torch.cat([output1, output2, output3, output4], 1))\n",
        "\n",
        "        # Return logits without softmax for CrossEntropyLoss\n",
        "        return output\n",
        "    \n",
        "########### SNUNet_ECAM ###########\n",
        "class conv_block_nested(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        identity = x\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        output = self.activation(x + identity)\n",
        "        return output\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2,\n",
        "                            mode='bilinear',\n",
        "                            align_corners=True) if bilinear else \\\n",
        "                 nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        reduced_channels = max(8, in_channels // ratio)\n",
        "        self.fc1 = nn.Conv2d(in_channels, reduced_channels, 1, bias=False)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Conv2d(reduced_channels, in_channels, 1, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
        "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class SNUNet_ECAM(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3):\n",
        "        super(SNUNet_ECAM, self).__init__()\n",
        "        n1 = 32\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 8]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Encoder\n",
        "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        self.Up1_0 = up(filters[1])\n",
        "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        self.Up2_0 = up(filters[2])\n",
        "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        self.Up3_0 = up(filters[3])\n",
        "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "        self.Up4_0 = up(filters[4])\n",
        "\n",
        "        # Decoder with correct input channels\n",
        "        self.conv0_1 = conv_block_nested(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_1 = up(filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_1 = up(filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] * 2 + filters[4], filters[3], filters[3])\n",
        "        self.Up3_1 = up(filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_2 = up(filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2] * 3 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_2 = up(filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1] * 4 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_3 = up(filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0] * 5 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        # Channel attention modules\n",
        "        self.ca = ChannelAttention(filters[0] * 4, ratio=16)\n",
        "        self.ca1 = ChannelAttention(filters[0], ratio=8)\n",
        "\n",
        "        # Final convolution\n",
        "        self.conv_final = nn.Sequential(\n",
        "            nn.Conv2d(filters[0] * 4, filters[0], kernel_size=1),\n",
        "            nn.BatchNorm2d(filters[0]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, xA, xB):\n",
        "        # Encoder path A\n",
        "        x0_0A = self.conv0_0(xA)\n",
        "        x1_0A = self.conv1_0(self.pool(x0_0A))\n",
        "        x2_0A = self.conv2_0(self.pool(x1_0A))\n",
        "        x3_0A = self.conv3_0(self.pool(x2_0A))\n",
        "\n",
        "        # Encoder path B\n",
        "        x0_0B = self.conv0_0(xB)\n",
        "        x1_0B = self.conv1_0(self.pool(x0_0B))\n",
        "        x2_0B = self.conv2_0(self.pool(x1_0B))\n",
        "        x3_0B = self.conv3_0(self.pool(x2_0B))\n",
        "        x4_0B = self.conv4_0(self.pool(x3_0B))\n",
        "\n",
        "        # Dense connections level 1\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n",
        "\n",
        "        # Dense connections level 2\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n",
        "\n",
        "        # Dense connections level 3\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n",
        "\n",
        "        # Final dense connections\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n",
        "\n",
        "        # Combine features\n",
        "        out = torch.cat([x0_1, x0_2, x0_3, x0_4], 1)\n",
        "        intra = torch.sum(torch.stack((x0_1, x0_2, x0_3, x0_4)), dim=0)\n",
        "\n",
        "        # Apply attention\n",
        "        ca1 = self.ca1(intra)\n",
        "        out = self.ca(out) * (out + ca1.repeat(1, 4, 1, 1))\n",
        "\n",
        "        # Final convolution\n",
        "        out = self.conv_final(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### All PCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "class ChangeDetectionModel(nn.Module):\n",
        "    def __init__(self, architecture='unet', encoder='resnet34', input_channels=3, num_semantic_classes=4, num_cd_classes=3):\n",
        "        super().__init__()\n",
        "        # Semantic segmentation models for each timestamp\n",
        "        if architecture.lower() == 'unet':\n",
        "            self.sem_model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=\"imagenet\",\n",
        "                in_channels=input_channels,\n",
        "                classes=num_semantic_classes,\n",
        "            )\n",
        "        elif architecture.lower() == 'linknet':\n",
        "            self.sem_model = smp.Linknet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=\"imagenet\",\n",
        "                in_channels=input_channels,\n",
        "                classes=num_semantic_classes,\n",
        "            )\n",
        "        elif architecture.lower() == 'pspnet':\n",
        "            self.sem_model = smp.PSPNet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=\"imagenet\",\n",
        "                in_channels=input_channels,\n",
        "                classes=num_semantic_classes,\n",
        "            )\n",
        "        elif architecture.lower() == 'deeplabv3plus':\n",
        "            self.sem_model = smp.DeepLabV3Plus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=\"imagenet\",\n",
        "                in_channels=input_channels,\n",
        "                classes=num_semantic_classes,\n",
        "            )\n",
        "\n",
        "        self.change_head = nn.Sequential(\n",
        "            nn.Conv2d(num_semantic_classes*2, 64, kernel_size=3, padding=1),  # Update input channels\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, num_cd_classes, kernel_size=1),  # Update output channels\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Get semantic features for both timestamps\n",
        "        sem_feat1 = self.sem_model(x1)\n",
        "        sem_feat2 = self.sem_model(x2)\n",
        "\n",
        "        # Concatenate semantic features\n",
        "        combined_feat = torch.cat([sem_feat1, sem_feat2], dim=1)\n",
        "\n",
        "        # Get change detection output\n",
        "        change_out = self.change_head(combined_feat)\n",
        "\n",
        "        return sem_feat1, sem_feat2, change_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### STANet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ModuleList\n",
        "\n",
        "class PAMBlock(nn.Module):\n",
        "    \"\"\"The basic implementation for self-attention block/non-local block\"\"\"\n",
        "    def __init__(self, in_channels, key_channels, value_channels, scale=1, ds=1):\n",
        "        super(PAMBlock, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.ds = ds\n",
        "        self.pool = nn.AvgPool2d(self.ds)\n",
        "        self.in_channels = in_channels\n",
        "        self.key_channels = key_channels\n",
        "        self.value_channels = value_channels\n",
        "\n",
        "        self.f_key = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n",
        "                     kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(self.key_channels)\n",
        "        )\n",
        "        self.f_query = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n",
        "                     kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(self.key_channels)\n",
        "        )\n",
        "        self.f_value = nn.Conv2d(in_channels=self.in_channels, out_channels=self.value_channels,\n",
        "                                kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input\n",
        "        if self.ds != 1:\n",
        "            x = self.pool(input)\n",
        "\n",
        "        batch_size, c, h, w = x.size(0), x.size(1), x.size(2), x.size(3) // 2\n",
        "\n",
        "        local_y = []\n",
        "        local_x = []\n",
        "        step_h, step_w = h // self.scale, w // self.scale\n",
        "        for i in range(0, self.scale):\n",
        "            for j in range(0, self.scale):\n",
        "                start_x, start_y = i * step_h, j * step_w\n",
        "                end_x, end_y = min(start_x + step_h, h), min(start_y + step_w, w)\n",
        "                if i == (self.scale - 1):\n",
        "                    end_x = h\n",
        "                if j == (self.scale - 1):\n",
        "                    end_y = w\n",
        "                local_x += [start_x, end_x]\n",
        "                local_y += [start_y, end_y]\n",
        "\n",
        "        value = self.f_value(x)\n",
        "        query = self.f_query(x)\n",
        "        key = self.f_key(x)\n",
        "\n",
        "        value = torch.stack([value[:, :, :, :w], value[:, :, :, w:]], 4)\n",
        "        query = torch.stack([query[:, :, :, :w], query[:, :, :, w:]], 4)\n",
        "        key = torch.stack([key[:, :, :, :w], key[:, :, :, w:]], 4)\n",
        "\n",
        "        local_block_cnt = 2 * self.scale * self.scale\n",
        "\n",
        "        def self_attention(value_local, query_local, key_local):\n",
        "            batch_size_new = value_local.size(0)\n",
        "            h_local, w_local = value_local.size(2), value_local.size(3)\n",
        "            value_local = value_local.contiguous().view(batch_size_new, self.value_channels, -1)\n",
        "\n",
        "            query_local = query_local.contiguous().view(batch_size_new, self.key_channels, -1)\n",
        "            query_local = query_local.permute(0, 2, 1)\n",
        "            key_local = key_local.contiguous().view(batch_size_new, self.key_channels, -1)\n",
        "\n",
        "            sim_map = torch.bmm(query_local, key_local)\n",
        "            sim_map = (self.key_channels ** -.5) * sim_map\n",
        "            sim_map = F.softmax(sim_map, dim=-1)\n",
        "\n",
        "            context_local = torch.bmm(value_local, sim_map.permute(0, 2, 1))\n",
        "            context_local = context_local.view(batch_size_new, self.value_channels, h_local, w_local, 2)\n",
        "            return context_local\n",
        "\n",
        "        v_list = [value[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]] for i in range(0, local_block_cnt, 2)]\n",
        "        v_locals = torch.cat(v_list, dim=0)\n",
        "        q_list = [query[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]] for i in range(0, local_block_cnt, 2)]\n",
        "        q_locals = torch.cat(q_list, dim=0)\n",
        "        k_list = [key[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]] for i in range(0, local_block_cnt, 2)]\n",
        "        k_locals = torch.cat(k_list, dim=0)\n",
        "        context_locals = self_attention(v_locals, q_locals, k_locals)\n",
        "\n",
        "        context_list = []\n",
        "        for i in range(0, self.scale):\n",
        "            row_tmp = []\n",
        "            for j in range(0, self.scale):\n",
        "                left = batch_size * (j + i * self.scale)\n",
        "                right = batch_size * (j + i * self.scale) + batch_size\n",
        "                tmp = context_locals[left:right]\n",
        "                row_tmp.append(tmp)\n",
        "            context_list.append(torch.cat(row_tmp, 3))\n",
        "\n",
        "        context = torch.cat(context_list, 2)\n",
        "        context = torch.cat([context[:, :, :, :, 0], context[:, :, :, :, 1]], 3)\n",
        "\n",
        "        if self.ds != 1:\n",
        "            context = F.interpolate(context, [h * self.ds, 2 * w * self.ds])\n",
        "\n",
        "        return context\n",
        "\n",
        "class PAM(nn.Module):\n",
        "    \"\"\"PAM (Position Attention Module)\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, sizes=([1]), ds=1):\n",
        "        super(PAM, self).__init__()\n",
        "        self.group = len(sizes)\n",
        "        self.stages = []\n",
        "        self.ds = ds\n",
        "        self.value_channels = out_channels\n",
        "        self.key_channels = out_channels // 8\n",
        "\n",
        "        self.stages = nn.ModuleList(\n",
        "            [PAMBlock(in_channels, self.key_channels, self.value_channels, size, self.ds)\n",
        "             for size in sizes])\n",
        "        self.conv_bn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * self.group, out_channels, kernel_size=1, padding=0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, feats):\n",
        "        priors = [stage(feats) for stage in self.stages]\n",
        "        context = []\n",
        "        for i in range(0, len(priors)):\n",
        "            context += [priors[i]]\n",
        "        output = self.conv_bn(torch.cat(context, 1))\n",
        "        return output\n",
        "\n",
        "class BAM(nn.Module):\n",
        "    \"\"\"Basic self-attention module\"\"\"\n",
        "    def __init__(self, in_dim, ds=8, activation=nn.ReLU):\n",
        "        super(BAM, self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.key_channel = self.chanel_in // 8\n",
        "        self.activation = activation\n",
        "        self.ds = ds\n",
        "        self.pool = nn.AvgPool2d(self.ds)\n",
        "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.pool(input)\n",
        "        m_batchsize, C, width, height = x.size()\n",
        "        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        energy = (self.key_channel ** -.5) * energy\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(m_batchsize, C, width, height)\n",
        "        out = F.interpolate(out, [width * self.ds, height * self.ds])\n",
        "        out = out + input\n",
        "        return out\n",
        "\n",
        "class CDSA(nn.Module):\n",
        "    \"\"\"Self attention module for change detection\"\"\"\n",
        "    def __init__(self, in_c, ds=1, mode='BAM'):\n",
        "        super(CDSA, self).__init__()\n",
        "        self.in_C = in_c\n",
        "        self.ds = ds\n",
        "        self.mode = mode\n",
        "        if self.mode == 'BAM':\n",
        "            self.Self_Att = BAM(self.in_C, ds=self.ds)\n",
        "        elif self.mode == 'PAM':\n",
        "            self.Self_Att = PAM(in_channels=self.in_C, out_channels=self.in_C, sizes=[1, 2, 4, 8], ds=self.ds)\n",
        "        elif self.mode == 'None':\n",
        "            self.Self_Att = nn.Identity()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        height = x1.shape[3]\n",
        "        x = torch.cat((x1, x2), 3)\n",
        "        x = self.Self_Att(x)\n",
        "        return x[:, :, :, 0:height], x[:, :, :, height:]\n",
        "\n",
        "class STANet(nn.Module):\n",
        "    \"\"\"STANet for multiclass change detection\"\"\"\n",
        "    def __init__(self, input_channels=3, hidden_channels=32, num_cd_classes=3, attention_mode='BAM'):\n",
        "        super(STANet, self).__init__()\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_cd_classes = num_cd_classes\n",
        "        self.attention_mode = attention_mode\n",
        "\n",
        "        # Encoder/Backbone layers\n",
        "        self.conv1 = nn.Conv2d(input_channels, hidden_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(hidden_channels)\n",
        "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(hidden_channels*2)\n",
        "        self.conv3 = nn.Conv2d(hidden_channels*2, hidden_channels*4, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(hidden_channels*4)\n",
        "\n",
        "        # Self-attention module\n",
        "        self.sa = CDSA(in_c=hidden_channels*4, ds=1, mode=attention_mode)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.upconv1 = nn.ConvTranspose2d(hidden_channels*8, hidden_channels*4, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.upbn1 = nn.BatchNorm2d(hidden_channels*4)\n",
        "        self.upconv2 = nn.ConvTranspose2d(hidden_channels*4, hidden_channels*2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.upbn2 = nn.BatchNorm2d(hidden_channels*2)\n",
        "        self.upconv3 = nn.ConvTranspose2d(hidden_channels*2, hidden_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.upbn3 = nn.BatchNorm2d(hidden_channels)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.final_conv = nn.Conv2d(hidden_channels, num_cd_classes, kernel_size=1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Encoder path\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x):\n",
        "        # Decoder path\n",
        "        x = F.relu(self.upbn1(self.upconv1(x)))\n",
        "        x = F.relu(self.upbn2(self.upconv2(x)))\n",
        "        x = F.relu(self.upbn3(self.upconv3(x)))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Encode both images\n",
        "        feat1 = self.encode(x1)\n",
        "        feat2 = self.encode(x2)\n",
        "\n",
        "        # Apply self-attention\n",
        "        att1, att2 = self.sa(feat1, feat2)\n",
        "\n",
        "        # Concatenate attended features\n",
        "        combined = torch.cat([att1, att2], dim=1)\n",
        "\n",
        "        # Decode\n",
        "        decoded = self.decode(combined)\n",
        "\n",
        "        # Final classification\n",
        "        out = self.final_conv(decoded)\n",
        "\n",
        "        # Only apply softmax during inference\n",
        "        if not self.training:\n",
        "            out = self.softmax(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Strategy 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "class Strategy3Model:\n",
        "    \"\"\"Combined CD and LCM model with checkpoint management\"\"\"\n",
        "    def __init__(self, cd_architecture='unet', lcm_architecture='unet',\n",
        "                 cd_encoder='resnet34', lcm_encoder='resnet34',\n",
        "                 input_channels=3, num_cd_classes=13, num_semantic_classes=4):\n",
        "        # Initialize CD model\n",
        "        self.cd_model = self._create_cd_model(\n",
        "            architecture=cd_architecture,\n",
        "            encoder=cd_encoder,\n",
        "            input_channels=input_channels\n",
        "        )\n",
        "        # Initialize LCM model\n",
        "        self.lcm_model = self._create_lcm_model(\n",
        "            architecture=lcm_architecture,\n",
        "            encoder=lcm_encoder,\n",
        "            input_channels=input_channels,\n",
        "            num_semantic_classes=num_semantic_classes\n",
        "        )\n",
        "\n",
        "    def _create_cd_model(self, architecture, encoder, input_channels):\n",
        "        \"\"\"Create binary change detection model\"\"\"\n",
        "        if architecture.lower() == 'unet':\n",
        "            model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights='imagenet',\n",
        "                in_channels=input_channels*2,  # Concatenated images\n",
        "                classes=1,  # Binary output,\n",
        "                encoder_depth=4,  # Reduce depth (def=5)\n",
        "                decoder_channels=(256, 128, 64, 32)  # Reduce channels(def=(256, 128, 64, 32, 16))\n",
        "\n",
        "            )\n",
        "        elif architecture.lower() == 'deeplabv3plus':\n",
        "            model = smp.DeepLabV3Plus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights='imagenet',\n",
        "                in_channels=input_channels*2,\n",
        "                classes=1,\n",
        "            )\n",
        "        # Add more architectures as needed\n",
        "        return model\n",
        "\n",
        "    def _create_lcm_model(self, architecture, encoder, input_channels, num_semantic_classes=4):\n",
        "        \"\"\"Create land cover mapping model\"\"\"\n",
        "        if architecture.lower() == 'unet':\n",
        "            model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights='imagenet',\n",
        "                in_channels=input_channels,\n",
        "                classes=num_semantic_classes,  # 4 land cover classes\n",
        "            )\n",
        "        elif architecture.lower() == 'deeplabv3plus':\n",
        "            model = smp.DeepLabV3Plus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights='imagenet',\n",
        "                in_channels=input_channels,\n",
        "                classes=num_semantic_classes,\n",
        "            )\n",
        "        # Add more architectures as needed\n",
        "        return model\n",
        "\n",
        "    def to(self, device):\n",
        "        \"\"\"Move models to device\"\"\"\n",
        "        self.cd_model = self.cd_model.to(device)\n",
        "        self.lcm_model = self.lcm_model.to(device)\n",
        "        return self\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Set models to training mode\"\"\"\n",
        "        self.cd_model.train()\n",
        "        self.lcm_model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Set models to evaluation mode\"\"\"\n",
        "        self.cd_model.eval()\n",
        "        self.lcm_model.eval()\n",
        "\n",
        "def create_semantic_change_mask(binary_pred, lcm_pred_2019, lcm_pred_2024):\n",
        "    \"\"\"Convert binary change + LCM predictions to 13-class semantic change mask.\n",
        "\n",
        "    Optimized version using vectorized operations and pre-computed lookup tables.\n",
        "\n",
        "    Args:\n",
        "        binary_pred: Binary change prediction tensor (B, 1, H, W)\n",
        "        lcm_pred_2019: Land cover prediction tensor for 2019 (B, C, H, W)\n",
        "        lcm_pred_2024: Land cover prediction tensor for 2024 (B, C, H, W)\n",
        "\n",
        "    Returns:\n",
        "        Semantic change mask tensor (B, H, W) with values 0-12\n",
        "    \"\"\"\n",
        "    device = binary_pred.device\n",
        "    batch_size = binary_pred.shape[0]\n",
        "    height = binary_pred.shape[2]\n",
        "    width = binary_pred.shape[3]\n",
        "\n",
        "    # Pre-compute land cover predictions - do this once\n",
        "    lcm_2019 = torch.argmax(lcm_pred_2019, dim=1)  # (B, H, W)\n",
        "    lcm_2024 = torch.argmax(lcm_pred_2024, dim=1)  # (B, H, W)\n",
        "\n",
        "    # Create the change mask - use threshold without squeeze/unsqueeze\n",
        "    change_mask = binary_pred[:, 0] > 0.5  # (B, H, W)\n",
        "\n",
        "    # Initialize output tensor\n",
        "    semantic_mask = torch.zeros((batch_size, height, width), device=device, dtype=torch.long)\n",
        "\n",
        "    # Create transition matrix lookup table - speeds up class mapping\n",
        "    # Format: from_class * num_cd_classes + to_class = semantic_class\n",
        "    num_cd_classes = 4  # Water, Building, Sparse, Dense\n",
        "    transitions = torch.full((num_cd_classes * num_cd_classes,), 0, device=device)\n",
        "\n",
        "    # Populate transition matrix - all transitions not listed default to 0 (no change)\n",
        "    transition_map = {\n",
        "        (0, 1): 1,   # Water → Building\n",
        "        (0, 2): 2,   # Water → Sparse\n",
        "        (0, 3): 3,   # Water → Dense\n",
        "        (1, 0): 4,   # Building → Water\n",
        "        (1, 2): 5,   # Building → Sparse\n",
        "        (1, 3): 6,   # Building → Dense\n",
        "        (2, 0): 7,   # Sparse → Water\n",
        "        (2, 1): 8,   # Sparse → Building\n",
        "        (2, 3): 9,   # Sparse → Dense\n",
        "        (3, 0): 10,  # Dense → Water\n",
        "        (3, 1): 11,  # Dense → Building\n",
        "        (3, 2): 12,  # Dense → Sparse\n",
        "    }\n",
        "\n",
        "    for (from_idx, to_idx), semantic_idx in transition_map.items():\n",
        "        transitions[from_idx * num_cd_classes + to_idx] = semantic_idx\n",
        "\n",
        "    # Vectorized computation of semantic classes\n",
        "    # Only compute for changed pixels to save memory\n",
        "    changed_pixels = change_mask.nonzero(as_tuple=True)\n",
        "    if len(changed_pixels[0]) > 0:\n",
        "        from_classes = lcm_2019[changed_pixels]  # (N,)\n",
        "        to_classes = lcm_2024[changed_pixels]    # (N,)\n",
        "\n",
        "        # Compute transition indices\n",
        "        transition_indices = from_classes * num_cd_classes + to_classes  # (N,)\n",
        "\n",
        "        # Look up semantic classes from transition matrix\n",
        "        semantic_classes = transitions[transition_indices]  # (N,)\n",
        "\n",
        "        # Assign semantic classes to output mask\n",
        "        semantic_mask[changed_pixels] = semantic_classes\n",
        "\n",
        "    return semantic_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Strategy 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiTaskChangeDetectionModel(nn.Module):\n",
        "    def __init__(self, input_channels, num_semantic_classes, num_change_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Shared Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Initial block\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Second block\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Land Cover Mapping Decoder (shared weights)\n",
        "        self.lcm_decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, num_semantic_classes, kernel_size=4, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        # Change Detection Decoder\n",
        "        self.cd_decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, num_change_classes, kernel_size=4, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Ensure input images are the same size\n",
        "        assert x1.shape == x2.shape, \"Input images must have the same dimensions\"\n",
        "\n",
        "        # Encode both images\n",
        "        enc1 = self.encoder(x1)\n",
        "        enc2 = self.encoder(x2)\n",
        "\n",
        "        # Land Cover Mapping for both time periods\n",
        "        lcm1 = self.lcm_decoder(enc1)\n",
        "        lcm2 = self.lcm_decoder(enc2)\n",
        "\n",
        "        # Change Detection (using difference of encodings)\n",
        "        cd_input = torch.abs(enc1 - enc2)  # Or torch.cat([enc1, enc2], dim=1)\n",
        "        cd_output = self.cd_decoder(cd_input)\n",
        "\n",
        "        return lcm1, lcm2, cd_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the checkpoint path, model name etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SiamUnets all architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Initialize and train the model\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# num_cd_classes = len(CLASSES)  #num classes in change mask\n",
        "# weighting_method = 'square_balanced'  #balanced,square_balanced,custom\n",
        "\n",
        "# weighted_metrics = True if num_cd_classes > 5 else False   #True for 13 classes,False for 3 classes\n",
        "\n",
        "# #13 classes\n",
        "# #strategy = 'st2' \n",
        "# #name = f\"{strategy}_{MODEL_NAME}-{num_cd_classes}_classes_{NUM_EPOCHS}\" #13 classes\n",
        "\n",
        "# #3 classes\n",
        "# strategy = 'st1' \n",
        "# name = f\"{strategy}_{MODEL_NAME}_{NUM_EPOCHS}\" \n",
        "\n",
        "# # Initialize model\n",
        "# checkpoint_path = f'{SAVING_DIR}/best_{name}.pt'\n",
        "\n",
        "# if MODEL_NAME == 'siamunet_conc':\n",
        "#     model = SiamUnet_conc(input_nbr=3, label_nbr=num_cd_classes).to(device)\n",
        "# elif MODEL_NAME == 'siamunet_diff':\n",
        "#     model = SiamUnet_diff(input_nbr=3, label_nbr=num_cd_classes).to(device)\n",
        "# elif MODEL_NAME == 'siamunet_EF':\n",
        "#     model = SiamUnet_EF(input_nbr=3, label_nbr=num_cd_classes).to(device)\n",
        "# elif MODEL_NAME == 'snunet_conc':\n",
        "#     model = Siam_NestedUNet_Conc(in_ch=3, out_ch=num_cd_classes).to(device)\n",
        "# elif MODEL_NAME == 'snunet_ECAM':\n",
        "#     model = SNUNet_ECAM(in_ch=3, out_ch=num_cd_classes).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCC all architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Initialize model and device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# num_cd_classes = len(CLASSES)   # Change Detection classes (3 for cd1, 13 for cd2)\n",
        "# num_semantic_classes = len(SEMANTIC_CLASSES)   # Semantic segmentation LCM classes (4 for both)\n",
        "# weighted_metrics = True if num_cd_classes > 5 else False   #True for 13 classes,False for 3 classes\n",
        "\n",
        "# #name = f'{ARCHITECTURE}-{num_cd_classes}_classes_{NUM_EPOCHS}'  #3 classes\n",
        "# name = f'{ARCHITECTURE}_{NUM_EPOCHS}'  #13 classes\n",
        "# checkpoint_path = f'{SAVING_DIR}/best_{name}_epochs.pt'\n",
        "\n",
        "# # Create model\n",
        "# model = ChangeDetectionModel(\n",
        "#     architecture=ARCHITECTURE,encoder='resnet34',\n",
        "#     input_channels=3,num_cd_classes=num_cd_classes,\n",
        "#     num_semantic_classes=num_semantic_classes\n",
        "# ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### STANet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Initialize and train the model\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model_name1 = 'stanet'\n",
        "# attention_mode = 'None' # 'BAM' 'PAM' 'None'\n",
        "# num_cd_classes = 3  #num classes in change mask\n",
        "# num_epochs = 100\n",
        "# weighting_method = 'square_balanced' #'custom'\n",
        "# loss = 'CE' #'focal' #'bcl'\n",
        "# checkpoint_path = f'{SAVING_DIR}/best_{model_name1}_{attention_mode}-{num_cd_classes}_classes_{num_epochs}.pt'\n",
        "\n",
        "# # Initialize model and data loaders\n",
        "# model = STANet(input_channels=3, hidden_channels=32, \n",
        "#                num_cd_classes=num_cd_classes, \n",
        "#                attention_mode=attention_mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Strategy 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_cd_classes=13   #Change Detection classes (3 for cd1, 13 for cd2)\n",
        "num_semantic_classes=4  #Semantic Segmentation LCM classes (4 for both)\n",
        "num_epochs = 100\n",
        "weighting_method = 'square_balanced'\n",
        "loss = 'CE' #'CE'\n",
        "checkpoint_path = f'{SAVING_DIR}/best_Strat3_{num_epochs}_epochs.pt'  #'models/strategy3_model.pt'\n",
        "\n",
        "# Create model\n",
        "model = Strategy3Model(\n",
        "    cd_architecture='unet',\n",
        "    lcm_architecture='unet',\n",
        "    cd_encoder='resnet34',\n",
        "    lcm_encoder='resnet34',\n",
        "    input_channels=3,\n",
        "    num_cd_classes=num_cd_classes,\n",
        "    num_semantic_classes=num_semantic_classes\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Strategy 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Model configuration\n",
        "input_channels = 3\n",
        "num_semantic_classes = 4 #len(SEMANTIC_CLASSES)\n",
        "num_change_classes = 13 #len(CLASSES)\n",
        "num_cd_classes = num_change_classes\n",
        "num_epochs = 100\n",
        "\n",
        "# Create model\n",
        "model = MultiTaskChangeDetectionModel(\n",
        "    input_channels=input_channels,\n",
        "    num_semantic_classes=num_semantic_classes,\n",
        "    num_change_classes=num_change_classes\n",
        ").to(device)\n",
        "\n",
        "# Define checkpoint paths\n",
        "lcm_checkpoint_path = f'{SAVING_DIR}/best_lcm_model_{num_epochs}.pt'\n",
        "full_checkpoint_path = f'{SAVING_DIR}/best_full_model_{num_epochs}.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa0RAqCdChmI"
      },
      "source": [
        "### Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def find_sample_idx_by_filename(test_loader, filename):\n",
        "    \"\"\"\n",
        "    Find the sample index for a given filename in the test_loader dataset.\n",
        "    \n",
        "    Args:\n",
        "        test_loader: DataLoader containing the test data\n",
        "        filename: Filename to search for (without path)\n",
        "    \n",
        "    Returns:\n",
        "        int: Index of the sample, or -1 if not found\n",
        "    \"\"\"\n",
        "    # Access the underlying dataset\n",
        "    dataset = test_loader.dataset\n",
        "    \n",
        "    # Search through all paths in the dataset\n",
        "    for idx in range(len(dataset)):\n",
        "        # Get the t2019 paths from your dataset (adjust this based on your dataset structure)\n",
        "        current_path = dataset.t2019_paths[idx]\n",
        "        current_filename = os.path.basename(current_path)\n",
        "        \n",
        "        # Check if this is the file we're looking for\n",
        "        if filename in current_filename:\n",
        "            return idx\n",
        "    \n",
        "    return -1\n",
        "\n",
        "def visualize_single_sample(model, test_loader, sample_idx, device='cpu', \n",
        "                          num_cd_classes=3, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize prediction for a specific sample from the test_loader.\n",
        "    \n",
        "    Args:\n",
        "        model: The model to use for prediction\n",
        "        test_loader: DataLoader containing the test data\n",
        "        sample_idx: Index of the sample to visualize\n",
        "        device: Device to run model on\n",
        "        num_cd_classes: Number of classes in change detection\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    ######## Siamunets, PCC, STANet #############\n",
        "    # checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "    \n",
        "    ########### STRATEGY 3 ################\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    model.cd_model.load_state_dict(checkpoint['cd_model'])\n",
        "    model.lcm_model.load_state_dict(checkpoint['lcm_model'])\n",
        "    print(\"Successfully loaded both CD and LCM models\")\n",
        "\n",
        "    ########### STRATEGY 4 ################\n",
        "    # checkpoint_path=full_checkpoint_path\n",
        "    # checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Get the specific sample\n",
        "    total_batches = len(test_loader)\n",
        "    batch_size = test_loader.batch_size\n",
        "    batch_idx = sample_idx // batch_size\n",
        "    item_idx = sample_idx % batch_size\n",
        "\n",
        "    if batch_idx >= total_batches:\n",
        "        print(f\"Sample index {sample_idx} is out of range. Maximum index is {total_batches * batch_size - 1}\")\n",
        "        return\n",
        "\n",
        "    # Get the specific batch\n",
        "    for i, (inputs1, inputs2, labels) in enumerate(test_loader):\n",
        "        if i == batch_idx:\n",
        "            break\n",
        "\n",
        "    # Extract the specific item from the batch\n",
        "    img1 = inputs1[item_idx]\n",
        "    img2 = inputs2[item_idx]\n",
        "    true_mask = labels[item_idx]\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        # Add batch dimension for model\n",
        "        img1_batch = img1.unsqueeze(0).to(device)\n",
        "        img2_batch = img2.unsqueeze(0).to(device)\n",
        "\n",
        "        ########### Siamunets, STANet ################\n",
        "        # output = model(img1_batch, img2_batch)\n",
        "        # pred_mask = torch.argmax(output, dim=1)[0].cpu().numpy()\n",
        "\n",
        "        ########### PCC, STRATEGY 4 ################\n",
        "        # segpred2019,segpred2024,output = model(img1_batch, img2_batch)\n",
        "        # pred_mask = torch.argmax(output, dim=1)[0].cpu().numpy()\n",
        "        # lcm_pred_2019 = torch.argmax(segpred2019, dim=1)[0].cpu().numpy()\n",
        "        # lcm_pred_2024 = torch.argmax(segpred2024, dim=1)[0].cpu().numpy()\n",
        "\n",
        "        ########### STRATEGY 3 ################\n",
        "        cd_pred = model.cd_model(torch.cat([img1.unsqueeze(0), img2.unsqueeze(0)], dim=1))\n",
        "        seg_pred_2019 = model.lcm_model(img1.unsqueeze(0))\n",
        "        seg_pred_2024 = model.lcm_model(img2.unsqueeze(0))\n",
        "        semantic_pred = create_semantic_change_mask(cd_pred, seg_pred_2019, seg_pred_2024)\n",
        "        pred_mask = semantic_pred.squeeze(0)\n",
        "        lcm_pred_2019 = torch.argmax(seg_pred_2019, dim=1)\n",
        "        lcm_pred_2024 = torch.argmax(seg_pred_2024, dim=1)\n",
        "    \n",
        "    colors_seg_2019 = ['lightblue','white','lightgreen', 'darkgreen']\n",
        "    colors_seg_2024 = ['white', 'lightblue','lightgreen', 'darkgreen']\n",
        "    cmap_seg_2019 = plt.matplotlib.colors.ListedColormap(colors_seg_2019)\n",
        "    cmap_seg_2024 = plt.matplotlib.colors.ListedColormap(colors_seg_2019)\n",
        "\n",
        "    # plt.imshow(lcm_pred_2019[0], cmap=cmap_seg_2019)\n",
        "    # plt.show()\n",
        "    plt.imsave(f\"{save_path}_lcm2019.png\", lcm_pred_2019[0], cmap=cmap_seg_2019)\n",
        "    plt.imsave(f\"{save_path}_lcm2024.png\", lcm_pred_2024[0], cmap=cmap_seg_2019)\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    plt.subplots_adjust(wspace=0.3)\n",
        "\n",
        "    # Display original images\n",
        "    img1_display = img1.numpy().transpose(1, 2, 0)\n",
        "    img2_display = img2.numpy().transpose(1, 2, 0)\n",
        "\n",
        "    axes[0].imshow(img1_display)\n",
        "    axes[0].set_title('Image 1')\n",
        "    axes[0].axis('off')\n",
        "    #plt.imsave(f\"{save_path}_img2019.png\", img1_display)\n",
        "\n",
        "    axes[1].imshow(img2_display)\n",
        "    axes[1].set_title('Image 2')\n",
        "    axes[1].axis('off')\n",
        "    #plt.imsave(f\"{save_path}_img2024.png\", img2_display)\n",
        "    \n",
        "    ########## Colors ##########\n",
        "    if num_cd_classes == 3:\n",
        "        colors = ['black', 'green', 'red']\n",
        "\n",
        "    elif num_cd_classes == 13:\n",
        "        colors = ['black','lightgray', 'gray', 'darkgray',\n",
        "              'darkblue', 'green', 'darkgreen',\n",
        "              'lightblue', 'orange', 'lightgreen',\n",
        "              'blue', 'orangered', 'peachpuff']\n",
        "\n",
        "    cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
        "\n",
        "    # Plot predicted and true masks\n",
        "    axes[2].imshow(pred_mask, cmap=cmap, vmin=0, vmax=num_cd_classes-1)\n",
        "    axes[2].set_title('Predicted Change')\n",
        "    axes[2].axis('off')\n",
        "    #plt.imsave(f\"{save_path}_CDprediction.png\", pred_mask, cmap=cmap, vmin=0, vmax=num_cd_classes-1)\n",
        "\n",
        "    axes[3].imshow(true_mask, cmap=cmap, vmin=0, vmax=num_cd_classes-1)\n",
        "    axes[3].set_title('Ground Truth')\n",
        "    axes[3].axis('off')\n",
        "    #plt.imsave(f\"{save_path}_CDtruth.png\", true_mask, cmap=cmap, vmin=0, vmax=num_cd_classes-1)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Find sample by filename and visualize\n",
        "filename = 'NorthCarolina_Charlotte_W_2019_2.tif'\n",
        "#filename = 'Alabama_Cleveland_SE_2019_2.tif'\n",
        "#filename = 'Netherlands_Rotterdam_N_2019_3.tif'\n",
        "editedfilename = filename.replace('.tif','')\n",
        "display_loader = test_loader  # Using image from test_loader for visualization\n",
        "model_name = 'Strategy3' #f'{model_name1}_{attention_mode}'#ARCHITECTURE #MODEL_NAME #Strategy3\n",
        "rooting_dir = f\"{SAVING_DIR}/{model_name}_{num_cd_classes}-classes_{editedfilename}\"\n",
        "\n",
        "if not os.path.exists(rooting_dir):\n",
        "    os.mkdir(rooting_dir)\n",
        "\n",
        "sample_idx = find_sample_idx_by_filename(display_loader, filename)\n",
        "\n",
        "if sample_idx >= 0:\n",
        "    visualize_single_sample(model, display_loader, sample_idx, \n",
        "                            device=device, num_cd_classes=num_cd_classes,\n",
        "                            save_path=f\"{rooting_dir}/{model_name}_{num_cd_classes}-classes_{editedfilename}_\")\n",
        "else:\n",
        "    print(f\"File {filename} not found in dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding good city tiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Image loader\n",
        "- Used for finding good images (for paper)\n",
        "- The `sample_index` is written in image title (used to find the name of the city)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set up the plot size and remove axes\n",
        "fig, axs = plt.subplots(4, 3, figsize=(8,8))\n",
        "\n",
        "for i in range(2):\n",
        "    j = random.randint(0, len(val_dataset) - 1)\n",
        "    # `j` is the sample_index (can be used further)\n",
        "    image1, image2, mask = val_dataset[j]\n",
        "    # Display images\n",
        "    axs[i, 0].imshow(image1.permute(1, 2, 0))\n",
        "    axs[i, 0].set_title(f\"Real 2019 {j}\")\n",
        "    axs[i, 0].axis(\"off\")\n",
        "\n",
        "    axs[i, 1].imshow(image2.permute(1, 2, 0))\n",
        "    axs[i, 1].set_title(f\"Real 2024\")\n",
        "    axs[i, 1].axis(\"off\")\n",
        "\n",
        "    axs[i, 2].imshow(mask, cmap=\"turbo\")\n",
        "    axs[i, 2].set_title(f\"CD Mask\")\n",
        "    axs[i, 2].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrive filename from `index`\n",
        "- use the sample index from above to get the filename from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "loader = 'test'\n",
        "image_dir = f\"ChangeDetectionMergedDividedSplit-tif3/{loader}/Images/T2019\"\n",
        "\n",
        "# Get sorted list of filenames\n",
        "image_filenames = sorted(os.listdir(image_dir))\n",
        "\n",
        "# Define the index\n",
        "idx = 771 # Change to your required index\n",
        "\n",
        "# Retrieve filename using index\n",
        "filename = image_filenames[idx]\n",
        "print(f\"Filename at index {idx}: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating images, masks for paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert entire TIF folder to PNG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import rasterio\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def tif_to_png(tif_path, png_path):\n",
        "    # Read the .tif file using rasterio\n",
        "    with rasterio.open(tif_path) as src:\n",
        "        # Read the image data into a NumPy array\n",
        "        array = src.read()\n",
        "        if array.shape[0] == 3:  # RGB image\n",
        "            array = np.moveaxis(array, 0, -1)  # Reorder dimensions to (H, W, C)\n",
        "        elif array.shape[0] == 1:  # Grayscale image\n",
        "            array = array[0]  # Remove the single-band dimension\n",
        "    \n",
        "    # Normalize the array to range [0, 255] for saving as PNG\n",
        "    array = array - array.min()\n",
        "    array = (array / array.max() * 255).astype(np.uint8)\n",
        "    \n",
        "    # Save the NumPy array as a .png image using Pillow\n",
        "    img = Image.fromarray(array)\n",
        "    img.save(png_path)\n",
        "    print(f\"Saved {png_path}\")\n",
        "\n",
        "def convert_all_tifs_in_folder(input_folder, output_folder):\n",
        "    # Ensure the output folder exists\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    \n",
        "    # List all .tif files in the input folder\n",
        "    tif_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
        "    \n",
        "    if not tif_files:\n",
        "        print(f\"No .tif files found in {input_folder}\")\n",
        "        return\n",
        "\n",
        "    # Convert each .tif file to .png and save in the output folder\n",
        "    for tif_file in tif_files:\n",
        "        tif_path = os.path.join(input_folder, tif_file)\n",
        "        png_path = os.path.join(output_folder, tif_file.replace(\".tif\", \".png\"))\n",
        "        tif_to_png(tif_path, png_path)\n",
        "\n",
        "# Example usage:\n",
        "directory1 = \"./Organized/SouthDakota_SiouxFalls_E\"\n",
        "directory2 = \"./Organized/SouthDakota_SiouxFalls_E\"\n",
        "convert_all_tifs_in_folder(directory1, directory2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display and save the image\n",
        "- Directly from `TIF` to `PNG`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "\n",
        "# Load the .tif image\n",
        "tif_path = \"./image_saver/test/m_NorthCarolina_Charlotte_W_2024_2.tif\"\n",
        "\n",
        "with rasterio.open(tif_path) as src:\n",
        "    image = src.read(1)  # Read the first band (for grayscale images)\n",
        "\n",
        "#colors = ['lightblue', 'white', 'lightgreen','darkgreen']  # Seg mask (all 4 classes present)\n",
        "colors = ['white', 'lightgreen','darkgreen']  # Seg mask (3 classes)\n",
        "#colors = ['black', 'green','red']  # cd1 - MCD mask (3 classes)\n",
        "# colors = ['black','lightgray', 'gray', 'darkgray',   # cd2 - SCD mask (13 classes)\n",
        "#               'darkblue', 'green', 'darkgreen',\n",
        "#               'lightblue', 'orange', 'lightgreen',\n",
        "#               'blue', 'orangered', 'peachpuff']\n",
        "cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
        "\n",
        "# Plot the image with a user-defined colormap\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(image, cmap=cmap)\n",
        "\n",
        "######## Adding a legend ########\n",
        "#import matplotlib.patches as mpatches   \n",
        "# Define the class labels\n",
        "# class_labels = ['0: no change', '1: water to building', '2: water to sparse', '3: water to dense',\n",
        "#                 '4: building to water', '5: building to sparse', '6: building to dense',\n",
        "#                 '7: sparse to water', '8: sparse to building', '9: sparse to dense',\n",
        "#                 '10: dense to water', '11: dense to building', '12: dense to sparse']\n",
        "\n",
        "# Create a list of patches to be shown in the legend\n",
        "#patches = [mpatches.Patch(color=colors[i], label=class_labels[i]) for i in range(len(class_labels))]\n",
        "\n",
        "# Add the legend to the plot\n",
        "#plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), ncol=4,loc='upper left', borderaxespad=0., prop={'family': 'Times New Roman'})\n",
        "#fig = plt.gcf()\n",
        "#fig.subplots_adjust(right=0.8)\n",
        "#fig.set_size_inches(1, 10)\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.imsave(tif_path.replace('.tif', '_colormap.png'), image, cmap=cmap)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting State vs Num Cities graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List all filenames in given folder\n",
        "- useful for counting cities "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_filenames(folder_path):\n",
        "    \"\"\"Returns a list of filenames in the given folder.\"\"\"\n",
        "    return [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "# Example usage\n",
        "folder_path = \"./new_eur\"  # Change this to your folder path\n",
        "filenames = get_filenames(folder_path)\n",
        "#print(type(filenames))\n",
        "\n",
        "\n",
        "filenames2 = list(filenames)\n",
        "filenames3 = []\n",
        "for file in filenames2: \n",
        "    # file = os.path.splitext(os.path.basename(root_directory+file1))[0]\n",
        "    # print(type(file))\n",
        "    file  = file.replace(\"cd1_m_\",\"\")\n",
        "    file = file.replace(\".tif\", \"\")\n",
        "    # file = file.replace(\"_1\",\"\")\n",
        "    # file = file.replace(\"_2\",\"\")\n",
        "    # file = file.replace(\"_3\",\"\")\n",
        "    # file = file.replace(\"_4\",\"\")\n",
        "    file = file[:-2]\n",
        "    if(file[-1]==\"_\"):\n",
        "        file = file[:-1]\n",
        "    filenames3.append(file)\n",
        "    # print(file)\n",
        "\n",
        "filenames4 = list(set(filenames3))\n",
        "filenames4.sort()\n",
        "\n",
        "print(len(filenames4))\n",
        "# for party in filenames4:\n",
        "#     print(party)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replace some filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "replacements = {\n",
        "    'Alabama_Dayton': 'Ohio_Dayton',\n",
        "    'Alabama_Cincinnati': 'Ohio_Cincinnati',\n",
        "    'Alabama_Toledo': 'Ohio_Toledo',\n",
        "    'Illinois_Greenbay': 'Wisconsin_Greenbay',\n",
        "    'Illinois_FortWayne': 'Indiana_FortWayne',\n",
        "    'Illinois_SouthBend': 'Indiana_SouthBend'\n",
        "}\n",
        "\n",
        "# Apply replacements\n",
        "filenames4_replaced = [replacements.get(name, name) for name in filenames4]\n",
        "\n",
        "#print(filenames4_replaced)\n",
        "\n",
        "filenames4_replaced.sort()\n",
        "# for party in filenames4_replaced:\n",
        "#     if party == \"Illinois_Greenbay\":\n",
        "#         print(party)\n",
        "#         print(\"bla\")\n",
        "\n",
        "print(len(filenames4_replaced))\n",
        "\n",
        "state_city_split = [name.split('_') for name in filenames4_replaced]\n",
        "print(state_city_split)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot bar graph for state vs num of cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the number of cities per state\n",
        "state_counts = Counter([state for state, city in state_city_split])\n",
        "\n",
        "# Extract states and their corresponding counts\n",
        "states = list(state_counts.keys())\n",
        "city_counts = list(state_counts.values())\n",
        "\n",
        "print(len(states))\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(15, 7))\n",
        "bars = plt.bar(states, city_counts, color='#9198db')\n",
        "plt.xlabel('Countries')\n",
        "plt.ylabel('Number of Cities')\n",
        "plt.title('Number of Cities per Country in Europe')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5939633,
          "sourceId": 9710583,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
