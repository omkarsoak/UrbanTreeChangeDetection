{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FHV3km-tCqoU",
        "outputId": "f6ad97ef-61f9-4512-c48f-275a62ecfe67"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !unzip '/content/drive/MyDrive/BTechProject/ChangeDetectionMergedDividedSplit-tif3.zip' -d '/content/ChangeDetectionMergedDividedSplit-tif'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install rasterio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT_DIRECTORY = \"ChangeDetectionMergedDividedSplit-tif\"\n",
        "SAVING_DIR = \"/content/drive/MyDrive/BTechProject\"\n",
        "CD_DIR = \"cd1_Output\"  #FOR STANET ALWAYS USE cd1_Output\n",
        "\n",
        "if CD_DIR == \"cd1_Output\":\n",
        "    CLASSES = ['no_change','vegetation_increase','vegetation_decrease']\n",
        "elif CD_DIR == \"cd2_Output\":\n",
        "    # CLASSES = ['no_change', 'water_building', 'water_sparse', 'water_dense',\n",
        "    #            'building_water', 'building_sparse', 'building_dense',\n",
        "    #            'sparse_water', 'sparse_building', 'sparse_dense',\n",
        "    #            'dense_water', 'dense_building', 'dense_sparse']\n",
        "    CLASSES = [\n",
        "    'no_change','water_built', 'water_bare', 'water_sparse', 'water_trees',\n",
        "    'water_crops', 'built_water', 'built_bare', 'built_sparse', 'built_trees',\n",
        "    'built_crops',  'bare_water',  'bare_built',  'bare_sparse',  'bare_trees',\n",
        "    'bare_crops',  'sparse_water',  'sparse_built',  'sparse_bare',\n",
        "    'sparse_trees',  'sparse_crops',  'trees_water',  'trees_built',\n",
        "    'trees_bare',  'trees_sparse',  'trees_crops',  'crops_water',\n",
        "    'crops_built', 'crops_bare',  'crops_sparse',  'crops_trees']\n",
        "\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 5\n",
        "MODEL_NAME = 'stanet'\n",
        "ATTENTION_MODE = 'BAM' # 'BAM' or 'PAM', 'None'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMowMJXZChmE"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRKZrxCtChmG",
        "outputId": "dedd9924-e566-4f0c-ba93-4ff76ed67e0a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import rasterio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class ChangeDetectionDatasetTIF(Dataset):\n",
        "    def __init__(self, t2019_dir, t2024_dir, mask_dir,classes, transform=None):\n",
        "        self.t2019_dir = t2019_dir\n",
        "        self.t2024_dir = t2024_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.classes = classes  # Change detection classes\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load all paths\n",
        "        self.t2019_paths = sorted([f for f in os.listdir(t2019_dir) if f.endswith('.tif')])\n",
        "        self.t2024_paths = sorted([f for f in os.listdir(t2024_dir) if f.endswith('.tif')])\n",
        "        self.mask_paths = sorted([f for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.t2019_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load images using rasterio\n",
        "        with rasterio.open(os.path.join(self.t2019_dir, self.t2019_paths[index])) as src:\n",
        "            img_t2019 = src.read(out_dtype=np.float32) / 255.0\n",
        "        with rasterio.open(os.path.join(self.t2024_dir, self.t2024_paths[index])) as src:\n",
        "            img_t2024 = src.read(out_dtype=np.float32) / 255.0\n",
        "        # Load masks\n",
        "        with rasterio.open(os.path.join(self.mask_dir, self.mask_paths[index])) as src:\n",
        "            cd_mask = src.read(1).astype(np.int64)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        img_t2019 = torch.from_numpy(img_t2019)\n",
        "        img_t2024 = torch.from_numpy(img_t2024)\n",
        "        cd_mask = torch.from_numpy(cd_mask)\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform is not None:\n",
        "            img_t2019 = self.transform(img_t2019)\n",
        "            img_t2024 = self.transform(img_t2024)\n",
        "\n",
        "        return img_t2019, img_t2024, cd_mask\n",
        "\n",
        "def describe_loader(loader_type):\n",
        "    img2019, img2024, cd_mask = next(iter(loader_type))\n",
        "    print(\"Batch size:\", loader_type.batch_size)\n",
        "    print(\"2019 Image Shape:\", img2019.shape)\n",
        "    print(\"2024 Image Shape:\", img2024.shape)\n",
        "    print(\"Change Mask Shape:\", cd_mask.shape)\n",
        "    print(\"Number of images:\", len(loader_type.dataset))\n",
        "    print(\"Classes:\", loader_type.dataset.classes)\n",
        "    print(\"Unique CD values:\", torch.unique(cd_mask))\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/train/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/train/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/train/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "val_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/val/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/val/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/val/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "test_dataset = ChangeDetectionDatasetTIF(\n",
        "    t2019_dir=f\"{ROOT_DIRECTORY}/test/Images/T2019\",\n",
        "    t2024_dir=f\"{ROOT_DIRECTORY}/test/Images/T2024\",\n",
        "    mask_dir=f\"{ROOT_DIRECTORY}/test/{CD_DIR}\",\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(\"------------Train-----------\")\n",
        "describe_loader(train_loader)\n",
        "print(\"------------Val------------\")\n",
        "describe_loader(val_loader)\n",
        "print(\"------------Test------------\")\n",
        "describe_loader(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpBgPG9nChmH"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "EkAWjfmNChmH",
        "outputId": "88e86275-12fa-4e4e-ba7a-d5ec867fdabe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set up the plot size and remove axes\n",
        "fig, axs = plt.subplots(5, 3, figsize=(10,10))\n",
        "\n",
        "for i in range(5):\n",
        "    j = random.randint(0, len(train_dataset) - 1)\n",
        "    image1, image2, mask = train_dataset[j]\n",
        "\n",
        "    # Display images\n",
        "    axs[i, 0].imshow(image1.permute(1, 2, 0))\n",
        "    axs[i, 0].set_title(f\"Real 2019\")\n",
        "    axs[i, 0].axis(\"off\")\n",
        "\n",
        "    axs[i, 1].imshow(image2.permute(1, 2, 0))\n",
        "    axs[i, 1].set_title(f\"Real 2024\")\n",
        "    axs[i, 1].axis(\"off\")\n",
        "\n",
        "    axs[i, 2].imshow(mask, cmap=\"turbo\")\n",
        "    print(np.unique(mask))\n",
        "    axs[i, 2].set_title(f\"CD Mask\")\n",
        "    axs[i, 2].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKI0wDXtChmI"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7C2m2w0E3Rb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ModuleList\n",
        "\n",
        "class PAMBlock(nn.Module):\n",
        "    \"\"\"The basic implementation for self-attention block/non-local block\"\"\"\n",
        "    def __init__(self, in_channels, key_channels, value_channels, scale=1, ds=1):\n",
        "        super(PAMBlock, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.ds = ds\n",
        "        self.pool = nn.AvgPool2d(self.ds)\n",
        "        self.in_channels = in_channels\n",
        "        self.key_channels = key_channels\n",
        "        self.value_channels = value_channels\n",
        "\n",
        "        self.f_key = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n",
        "                     kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(self.key_channels)\n",
        "        )\n",
        "        self.f_query = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n",
        "                     kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(self.key_channels)\n",
        "        )\n",
        "        self.f_value = nn.Conv2d(in_channels=self.in_channels, out_channels=self.value_channels,\n",
        "                                kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input\n",
        "        if self.ds != 1:\n",
        "            x = self.pool(input)\n",
        "\n",
        "        batch_size, c, h, w = x.size(0), x.size(1), x.size(2), x.size(3) // 2\n",
        "\n",
        "        local_y = []\n",
        "        local_x = []\n",
        "        step_h, step_w = h // self.scale, w // self.scale\n",
        "        for i in range(0, self.scale):\n",
        "            for j in range(0, self.scale):\n",
        "                start_x, start_y = i * step_h, j * step_w\n",
        "                end_x, end_y = min(start_x + step_h, h), min(start_y + step_w, w)\n",
        "                if i == (self.scale - 1):\n",
        "                    end_x = h\n",
        "                if j == (self.scale - 1):\n",
        "                    end_y = w\n",
        "                local_x += [start_x, end_x]\n",
        "                local_y += [start_y, end_y]\n",
        "\n",
        "        value = self.f_value(x)\n",
        "        query = self.f_query(x)\n",
        "        key = self.f_key(x)\n",
        "\n",
        "        value = torch.stack([value[:, :, :, :w], value[:, :, :, w:]], 4)\n",
        "        query = torch.stack([query[:, :, :, :w], query[:, :, :, w:]], 4)\n",
        "        key = torch.stack([key[:, :, :, :w], key[:, :, :, w:]], 4)\n",
        "\n",
        "        local_block_cnt = 2 * self.scale * self.scale\n",
        "\n",
        "        def self_attention(value_local, query_local, key_local):\n",
        "            batch_size_new = value_local.size(0)\n",
        "            h_local, w_local = value_local.size(2), value_local.size(3)\n",
        "            value_local = value_local.contiguous().view(batch_size_new, self.value_channels, -1)\n",
        "\n",
        "            query_local = query_local.contiguous().view(batch_size_new, self.key_channels, -1)\n",
        "            query_local = query_local.permute(0, 2, 1)\n",
        "            key_local = key_local.contiguous().view(batch_size_new, self.key_channels, -1)\n",
        "\n",
        "            sim_map = torch.bmm(query_local, key_local)\n",
        "            sim_map = (self.key_channels ** -.5) * sim_map\n",
        "            sim_map = F.softmax(sim_map, dim=-1)\n",
        "\n",
        "            context_local = torch.bmm(value_local, sim_map.permute(0, 2, 1))\n",
        "            context_local = context_local.view(batch_size_new, self.value_channels, h_local, w_local, 2)\n",
        "            return context_local\n",
        "\n",
        "        v_list = [value[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]] for i in range(0, local_block_cnt, 2)]\n",
        "        v_locals = torch.cat(v_list, dim=0)\n",
        "        q_list = [query[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]] for i in range(0, local_block_cnt, 2)]\n",
        "        q_locals = torch.cat(q_list, dim=0)\n",
        "        k_list = [key[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]] for i in range(0, local_block_cnt, 2)]\n",
        "        k_locals = torch.cat(k_list, dim=0)\n",
        "        context_locals = self_attention(v_locals, q_locals, k_locals)\n",
        "\n",
        "        context_list = []\n",
        "        for i in range(0, self.scale):\n",
        "            row_tmp = []\n",
        "            for j in range(0, self.scale):\n",
        "                left = batch_size * (j + i * self.scale)\n",
        "                right = batch_size * (j + i * self.scale) + batch_size\n",
        "                tmp = context_locals[left:right]\n",
        "                row_tmp.append(tmp)\n",
        "            context_list.append(torch.cat(row_tmp, 3))\n",
        "\n",
        "        context = torch.cat(context_list, 2)\n",
        "        context = torch.cat([context[:, :, :, :, 0], context[:, :, :, :, 1]], 3)\n",
        "\n",
        "        if self.ds != 1:\n",
        "            context = F.interpolate(context, [h * self.ds, 2 * w * self.ds])\n",
        "\n",
        "        return context\n",
        "\n",
        "class PAM(nn.Module):\n",
        "    \"\"\"PAM (Position Attention Module)\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, sizes=([1]), ds=1):\n",
        "        super(PAM, self).__init__()\n",
        "        self.group = len(sizes)\n",
        "        self.stages = []\n",
        "        self.ds = ds\n",
        "        self.value_channels = out_channels\n",
        "        self.key_channels = out_channels // 8\n",
        "\n",
        "        self.stages = nn.ModuleList(\n",
        "            [PAMBlock(in_channels, self.key_channels, self.value_channels, size, self.ds)\n",
        "             for size in sizes])\n",
        "        self.conv_bn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * self.group, out_channels, kernel_size=1, padding=0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, feats):\n",
        "        priors = [stage(feats) for stage in self.stages]\n",
        "        context = []\n",
        "        for i in range(0, len(priors)):\n",
        "            context += [priors[i]]\n",
        "        output = self.conv_bn(torch.cat(context, 1))\n",
        "        return output\n",
        "\n",
        "class BAM(nn.Module):\n",
        "    \"\"\"Basic self-attention module\"\"\"\n",
        "    def __init__(self, in_dim, ds=8, activation=nn.ReLU):\n",
        "        super(BAM, self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.key_channel = self.chanel_in // 8\n",
        "        self.activation = activation\n",
        "        self.ds = ds\n",
        "        self.pool = nn.AvgPool2d(self.ds)\n",
        "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.pool(input)\n",
        "        m_batchsize, C, width, height = x.size()\n",
        "        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        energy = (self.key_channel ** -.5) * energy\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(m_batchsize, C, width, height)\n",
        "        out = F.interpolate(out, [width * self.ds, height * self.ds])\n",
        "        out = out + input\n",
        "        return out\n",
        "\n",
        "class CDSA(nn.Module):\n",
        "    \"\"\"Self attention module for change detection\"\"\"\n",
        "    def __init__(self, in_c, ds=1, mode='BAM'):\n",
        "        super(CDSA, self).__init__()\n",
        "        self.in_C = in_c\n",
        "        self.ds = ds\n",
        "        self.mode = mode\n",
        "        if self.mode == 'BAM':\n",
        "            self.Self_Att = BAM(self.in_C, ds=self.ds)\n",
        "        elif self.mode == 'PAM':\n",
        "            self.Self_Att = PAM(in_channels=self.in_C, out_channels=self.in_C, sizes=[1, 2, 4, 8], ds=self.ds)\n",
        "        elif self.mode == 'None':\n",
        "            self.Self_Att = nn.Identity()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        height = x1.shape[3]\n",
        "        x = torch.cat((x1, x2), 3)\n",
        "        x = self.Self_Att(x)\n",
        "        return x[:, :, :, 0:height], x[:, :, :, height:]\n",
        "\n",
        "class STANet(nn.Module):\n",
        "    \"\"\"STANet for multiclass change detection\"\"\"\n",
        "    def __init__(self, input_channels=3, hidden_channels=32, num_cd_classes=3, attention_mode='BAM'):\n",
        "        super(STANet, self).__init__()\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_cd_classes = num_cd_classes\n",
        "        self.attention_mode = attention_mode\n",
        "\n",
        "        # Encoder/Backbone layers\n",
        "        self.conv1 = nn.Conv2d(input_channels, hidden_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(hidden_channels)\n",
        "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(hidden_channels*2)\n",
        "        self.conv3 = nn.Conv2d(hidden_channels*2, hidden_channels*4, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(hidden_channels*4)\n",
        "\n",
        "        # Self-attention module\n",
        "        self.sa = CDSA(in_c=hidden_channels*4, ds=1, mode=attention_mode)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.upconv1 = nn.ConvTranspose2d(hidden_channels*8, hidden_channels*4, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.upbn1 = nn.BatchNorm2d(hidden_channels*4)\n",
        "        self.upconv2 = nn.ConvTranspose2d(hidden_channels*4, hidden_channels*2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.upbn2 = nn.BatchNorm2d(hidden_channels*2)\n",
        "        self.upconv3 = nn.ConvTranspose2d(hidden_channels*2, hidden_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.upbn3 = nn.BatchNorm2d(hidden_channels)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.final_conv = nn.Conv2d(hidden_channels, num_cd_classes, kernel_size=1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Encoder path\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x):\n",
        "        # Decoder path\n",
        "        x = F.relu(self.upbn1(self.upconv1(x)))\n",
        "        x = F.relu(self.upbn2(self.upconv2(x)))\n",
        "        x = F.relu(self.upbn3(self.upconv3(x)))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Encode both images\n",
        "        feat1 = self.encode(x1)\n",
        "        feat2 = self.encode(x2)\n",
        "\n",
        "        # Apply self-attention\n",
        "        att1, att2 = self.sa(feat1, feat2)\n",
        "\n",
        "        # Concatenate attended features\n",
        "        combined = torch.cat([att1, att2], dim=1)\n",
        "\n",
        "        # Decode\n",
        "        decoded = self.decode(combined)\n",
        "\n",
        "        # Final classification\n",
        "        out = self.final_conv(decoded)\n",
        "\n",
        "        # Only apply softmax during inference\n",
        "        if not self.training:\n",
        "            out = self.softmax(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0-zY8rUE3Rc"
      },
      "source": [
        "## Util Functions and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Mk8d2vUXwH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "def calculate_effective_weights(train_loader, device, num_cd_classes=3, method='square_balanced'):\n",
        "    \"\"\"Calculate class weights with different strategies to handle class imbalance\n",
        "\n",
        "    Args:\n",
        "        train_loader: DataLoader containing training data\n",
        "        device: torch device\n",
        "        num_cd_classes: number of classes (default: 3)\n",
        "        method: weighting strategy ('balanced', 'square_balanced', or 'custom')\n",
        "    \"\"\"\n",
        "    class_counts = torch.zeros(num_cd_classes)\n",
        "    total_pixels = 0\n",
        "\n",
        "    # Count class frequencies\n",
        "    for _, _, labels in train_loader:\n",
        "        labels = labels.to(device)\n",
        "        for i in range(num_cd_classes):\n",
        "            class_counts[i] += (labels == i).sum().item()\n",
        "        total_pixels += labels.numel()\n",
        "\n",
        "    class_frequencies = class_counts / total_pixels\n",
        "\n",
        "    if method == 'balanced':\n",
        "        # Standard balanced weighting (inverse frequency)\n",
        "        weights = 1.0 / class_frequencies\n",
        "\n",
        "    elif method == 'square_balanced':\n",
        "        # Square root of inverse frequencies (less aggressive balancing)\n",
        "        weights = torch.sqrt(1.0 / class_frequencies)\n",
        "\n",
        "    elif method == 'custom':\n",
        "        # Custom weighting that maintains some natural class distribution\n",
        "        # Adjust these factors based on your domain knowledge\n",
        "        base_weights = 1.0 / class_frequencies\n",
        "        adjustment_factors = torch.tensor([0.7, 1.2, 1.2])\n",
        "        weights = base_weights * adjustment_factors\n",
        "\n",
        "    # Normalize weights to sum to num_cd_classes\n",
        "    weights = weights * (num_cd_classes / weights.sum())\n",
        "\n",
        "    return weights, class_frequencies\n",
        "\n",
        "def calculate_metrics(outputs, labels, num_cd_classes=3, weighted_metrics=False):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive metrics for change detection using a single confusion matrix\n",
        "\n",
        "    Args:\n",
        "        outputs (torch.Tensor or np.array): Model outputs or predictions\n",
        "        labels (torch.Tensor or np.array): Ground truth class labels\n",
        "        num_cd_classes (int): Number of classes in the dataset\n",
        "\n",
        "    Returns:\n",
        "        list: List of overall performance metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to numpy if inputs are torch tensors\n",
        "    if torch.is_tensor(outputs):\n",
        "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    else:\n",
        "        predictions = outputs\n",
        "\n",
        "    if torch.is_tensor(labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Flatten predictions and targets\n",
        "    pred_flat = predictions.flatten()\n",
        "    target_flat = labels.flatten()\n",
        "\n",
        "    # Compute confusion matrix once\n",
        "    cm = confusion_matrix(target_flat, pred_flat, labels=list(range(num_cd_classes)))\n",
        "\n",
        "    # Calculate metrics from confusion matrix\n",
        "    metrics = {}\n",
        "\n",
        "    # True positives, false positives, false negatives for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = np.sum(cm, axis=0) - tp\n",
        "    fn = np.sum(cm, axis=1) - tp\n",
        "\n",
        "    # Overall accuracy from confusion matrix\n",
        "    metrics['accuracy'] = np.sum(tp) / np.sum(cm)\n",
        "\n",
        "    # Per-class precision, recall, F1\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall = tp / (tp + fn + 1e-6)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
        "\n",
        "    if weighted_metrics == False:\n",
        "        # Unweighted averages\n",
        "        metrics['precision'] = np.average(precision)\n",
        "        metrics['recall'] = np.average(recall)\n",
        "        metrics['f1_score'] = np.average(f1)\n",
        "    elif weighted_metrics == True:\n",
        "        # Weighted averages\n",
        "        total = np.sum(cm, axis=1)\n",
        "        metrics['precision'] = np.average(precision,weights=total)\n",
        "        metrics['recall'] = np.average(recall, weights=total)\n",
        "        metrics['f1_score'] = np.average(f1,weights=total)\n",
        "\n",
        "    # Calculate Kappa directly from confusion matrix\n",
        "    n = np.sum(cm)\n",
        "    sum_po = np.sum(np.diag(cm))\n",
        "    sum_pe = np.sum(np.sum(cm, axis=0) * np.sum(cm, axis=1)) / n\n",
        "    metrics['kappa'] = (sum_po - sum_pe) / (n - sum_pe + 1e-6)\n",
        "\n",
        "    # IoU from confusion matrix\n",
        "    iou_per_class = tp / (tp + fp + fn + 1e-6)\n",
        "    metrics['miou'] = np.mean(iou_per_class)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=50, num_cd_classes=3,\n",
        "                device='cuda', learning_rate=1e-4, weight_decay=0.01,\n",
        "                checkpoint_path='best_stanet_model.pt', \n",
        "                weighting_method='square_balanced', weighted_metrics=False):\n",
        "    \"\"\"\n",
        "    Training function for STANet model with comprehensive metrics tracking.\n",
        "\n",
        "    Args:\n",
        "        model: STANet model instance\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        num_epochs: Number of training epochs\n",
        "        num_cd_classes: Number of classes for change detection\n",
        "        device: Device to run training on\n",
        "        learning_rate: Initial learning rate\n",
        "        weight_decay: Weight decay for optimizer\n",
        "        checkpoint_path: Path to save best model checkpoint\n",
        "    \"\"\"\n",
        "    # Initialize starting values\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Initialize metrics history\n",
        "    history = {\n",
        "        'train': {\n",
        "            'loss': [], 'accuracy': [], 'precision': [],\n",
        "            'recall': [], 'f1_score': [], 'miou': [], 'kappa': []\n",
        "        },\n",
        "        'val': {\n",
        "            'loss': [], 'accuracy': [], 'precision': [],\n",
        "            'recall': [], 'f1_score': [], 'miou': [], 'kappa': []\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            best_val_loss = checkpoint['best_val_loss']\n",
        "            history = checkpoint['history']\n",
        "            print(f\"Resuming from epoch {start_epoch} with best val loss: {best_val_loss:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting training from scratch\")\n",
        "\n",
        "    # Setup optimizer and losses\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    class_weights, _ = calculate_effective_weights(train_loader, device, num_cd_classes=num_cd_classes, method=weighting_method)\n",
        "    print(class_weights)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    def process_epoch(phase, data_loader):\n",
        "        \"\"\"Process one epoch of training or validation\"\"\"\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_metrics = {\n",
        "            'loss': 0.0, 'accuracy': 0.0, 'precision': 0.0,\n",
        "            'recall': 0.0, 'f1_score': 0.0, 'miou': 0.0, 'kappa': 0.0\n",
        "        }\n",
        "        samples_count = 0\n",
        "\n",
        "        # Use tqdm for progress bar\n",
        "        pbar = tqdm(data_loader, desc=f'{phase.capitalize()} Epoch')\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "            for inputs1, inputs2, labels in pbar:\n",
        "                # Move data to device\n",
        "                inputs1 = inputs1.to(device)\n",
        "                inputs2 = inputs2.to(device)\n",
        "                labels = labels.to(device)\n",
        "                batch_size = inputs1.size(0)\n",
        "\n",
        "                # Zero gradients for training\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs1, inputs2)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass for training\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    # Gradient clipping\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Calculate metrics\n",
        "                batch_metrics = calculate_metrics(outputs, labels, num_cd_classes=num_cd_classes, \n",
        "                                                  weighted_metrics=weighted_metrics)\n",
        "                batch_metrics['loss'] = loss.item()\n",
        "\n",
        "                # Update running metrics\n",
        "                for key in running_metrics:\n",
        "                    running_metrics[key] += batch_metrics[key] * batch_size\n",
        "                samples_count += batch_size\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f\"{batch_metrics['loss']:.4f}\",\n",
        "                    'miou': f\"{batch_metrics['miou']:.4f}\"\n",
        "                })\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_metrics = {key: value / samples_count for key, value in running_metrics.items()}\n",
        "\n",
        "        # Store metrics in history\n",
        "        for key in history[phase]:\n",
        "            history[phase][key].append(epoch_metrics[key])\n",
        "\n",
        "        return epoch_metrics\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        print(f'\\nEpoch {epoch + 1}/{num_epochs}:')\n",
        "\n",
        "        # Training phase\n",
        "        train_metrics = process_epoch('train', train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = process_epoch('val', val_loader)\n",
        "\n",
        "        # Print metrics\n",
        "        def print_metrics(phase, metrics):\n",
        "            print(f'\\n{phase.capitalize()} Metrics:')\n",
        "            print(f\"  Loss: {metrics['loss']:.4f}\")\n",
        "            print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "            print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "            print(f\"  Recall: {metrics['recall']:.4f}\")\n",
        "            print(f\"  F1-score: {metrics['f1_score']:.4f}\")\n",
        "            print(f\"  mIoU: {metrics['miou']:.4f}\")\n",
        "            print(f\"  Kappa: {metrics['kappa']:.4f}\")\n",
        "\n",
        "        print_metrics('train', train_metrics)\n",
        "        print_metrics('val', val_metrics)\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(val_metrics['loss'])\n",
        "\n",
        "        # Save checkpoint if it's the best model\n",
        "        if val_metrics['loss'] < best_val_loss:\n",
        "            best_val_loss = val_metrics['loss']\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'metrics': val_metrics,\n",
        "                'history': history\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f'\\nSaved new best model with validation loss: {val_metrics[\"loss\"]:.4f}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def save_training_files(history, checkpoint_path, history_filename, bestepoch_filename):\n",
        "    \"\"\"Save training history and best epoch info to separate JSON files\"\"\"\n",
        "\n",
        "    def convert_to_serializable(value):\n",
        "        \"\"\"Recursively convert numpy/torch types to basic Python types\"\"\"\n",
        "        if isinstance(value, (np.ndarray, torch.Tensor)):\n",
        "            return value.tolist()\n",
        "        elif isinstance(value, dict):\n",
        "            return {k: convert_to_serializable(v) for k, v in value.items()}\n",
        "        elif isinstance(value, list):\n",
        "            return [convert_to_serializable(item) for item in value]\n",
        "        return value\n",
        "\n",
        "    history_data = {\n",
        "        phase: {\n",
        "            metric: convert_to_serializable(values)\n",
        "            for metric, values in metrics.items()\n",
        "        }\n",
        "        for phase, metrics in history.items()\n",
        "    }\n",
        "\n",
        "    with open(history_filename, 'w') as f:\n",
        "        json.dump(history_data, f, indent=4)\n",
        "\n",
        "    # Load checkpoint without weights_only flag\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    # print(\"\\nCheckpoint contents:\")\n",
        "    # for key in checkpoint.keys():\n",
        "    #     print(f\"- {key}\")\n",
        "\n",
        "    # Convert metrics to basic Python types\n",
        "    epoch_data = {\n",
        "        'best_epoch': checkpoint['epoch'],\n",
        "        'best_val_loss': checkpoint['best_val_loss'],\n",
        "        'val_metrics': convert_to_serializable(checkpoint['metrics'])\n",
        "    }\n",
        "\n",
        "    with open(bestepoch_filename, 'w') as f:\n",
        "        json.dump(epoch_data, f, indent=4)\n",
        "\n",
        "    print(f\"\\nSaved training history to: {history_filename}\")\n",
        "    print(f\"Saved best epoch info to: {bestepoch_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2RI6l8bUXwH"
      },
      "source": [
        "## Model Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EayeD9l0u2V",
        "outputId": "a2cf3bb6-b622-4401-87f8-ff10b7045de7"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_cd_classes = len(CLASSES)  #num classes in change mask\n",
        "\n",
        "weighting_method = 'square_balanced'  #balanced,square_balanced,custom\n",
        "\n",
        "weighted_metrics = True if num_cd_classes > 5 else False   #True for 13 classes,False for 3 classes\n",
        "\n",
        "name = f\"{MODEL_NAME}_{ATTENTION_MODE}-{num_cd_classes}_classes_{NUM_EPOCHS}\"\n",
        "checkpoint_path = f'{SAVING_DIR}/best_{name}.pt'\n",
        "\n",
        "# Initialize model and data loaders\n",
        "model = STANet(input_channels=3, hidden_channels=32, \n",
        "               num_cd_classes=num_cd_classes, \n",
        "               attention_mode=ATTENTION_MODE)\n",
        "\n",
        "# Train model\n",
        "model, history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,val_loader=val_loader,\n",
        "    num_epochs=NUM_EPOCHS,num_cd_classes=num_cd_classes,\n",
        "    device=device,\n",
        "    learning_rate=1e-4,weight_decay=0.01,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    weighting_method=weighting_method,\n",
        "    weighted_metrics=weighted_metrics\n",
        ")\n",
        "\n",
        "history_filename = f\"{SAVING_DIR}/{name}_history.json\"\n",
        "bestepoch_filename = f\"{SAVING_DIR}/{name}_best_epoch.json\"\n",
        "save_training_files(history=history,checkpoint_path=checkpoint_path,\n",
        "                    history_filename=history_filename,bestepoch_filename=bestepoch_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa0RAqCdChmI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i6ngPdKERbCE",
        "outputId": "a75589bb-e6e2-495a-bd96-c4e121456aec"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def test_model(model, test_loader, device='cuda',\n",
        "               num_cd_classes=3, weighting_method='square_balanced',\n",
        "               weighted_metrics=False, checkpoint_path='best_stanet_model.pt'):\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    model = model.to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights, _ = calculate_effective_weights(test_loader, device,\n",
        "                                                   num_cd_classes=num_cd_classes,\n",
        "                                                   method=weighting_method)\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "    # For visualization and metrics\n",
        "    random_samples = []\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Collect predictions and labels for comprehensive metrics\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs1, inputs2, labels in test_loader:\n",
        "            inputs1 = inputs1.to(device) \n",
        "            inputs2 = inputs2.to(device) \n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs1, inputs2)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item() * inputs1.size(0)\n",
        "            total_samples += inputs1.size(0)\n",
        "\n",
        "            # Get predictions\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Store predictions and labels\n",
        "            all_predictions.append(preds.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "            # Store random samples for visualization\n",
        "            if len(random_samples) < 5:\n",
        "                for i in range(min(inputs1.size(0), 5 - len(random_samples))):\n",
        "                    if random.random() < 0.2:  # 20% chance to select each sample\n",
        "                        random_samples.append({\n",
        "                            'image1': inputs1[i].cpu(),\n",
        "                            'image2': inputs2[i].cpu(),\n",
        "                            'label': labels[i].cpu(),\n",
        "                            'pred': preds[i].cpu(),\n",
        "                            'probabilities': torch.softmax(outputs[i], dim=0).cpu()\n",
        "                        })\n",
        "\n",
        "    # Concatenate predictions and labels\n",
        "    all_predictions = np.concatenate(all_predictions)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_metrics = calculate_metrics(all_predictions, all_labels, num_cd_classes, \n",
        "                                     weighted_metrics=weighted_metrics)\n",
        "\n",
        "    # Add loss to metrics\n",
        "    test_metrics['loss'] = total_loss / total_samples\n",
        "\n",
        "    # Make sure we have exactly 5 samples\n",
        "    while len(random_samples) < 5:\n",
        "        random_samples.append(random_samples[-1] if random_samples else {\n",
        "            'image1': torch.zeros(3, 64, 64),\n",
        "            'image2': torch.zeros(3, 64, 64),\n",
        "            'label': torch.zeros(64, 64),\n",
        "            'pred': torch.zeros(64, 64),\n",
        "            'probabilities': torch.zeros(3, 64, 64)\n",
        "        })\n",
        "\n",
        "    return random_samples, test_metrics\n",
        "\n",
        "def visualize_results(random_samples, num_cd_classes=3):\n",
        "    # Extract samples and metrics\n",
        "    # random_samples = random_samples_and_metrics[0]\n",
        "    # test_metrics = random_samples_and_metrics[1]\n",
        "\n",
        "    # Create a figure with subplots\n",
        "    fig, axes = plt.subplots(5, 4, figsize=(25, 25))\n",
        "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "    for idx, sample in enumerate(random_samples):\n",
        "        # Normalize and convert images for display\n",
        "        img1 = sample['image1'].numpy().transpose(1, 2, 0)\n",
        "        img2 = sample['image2'].numpy().transpose(1, 2, 0)\n",
        "        img1 = (img1 - img1.min()) / (img1.max() - img1.min())\n",
        "        img2 = (img2 - img2.min()) / (img2.max() - img2.min())\n",
        "\n",
        "        # Get masks\n",
        "        pred_mask = sample['pred'].numpy()\n",
        "        true_mask = sample['label'].numpy()\n",
        "\n",
        "        # Plot images and masks\n",
        "        axes[idx, 0].imshow(img1)\n",
        "        axes[idx, 0].set_title('Image 1')\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        axes[idx, 1].imshow(img2)\n",
        "        axes[idx, 1].set_title('Image 2')\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Plot predicted mask\n",
        "        pred_plot = axes[idx, 2].imshow(pred_mask, cmap='tab10', vmin=0, vmax=num_cd_classes-1)\n",
        "        axes[idx, 2].set_title('Predicted Change')\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "        # Plot ground truth mask\n",
        "        true_plot = axes[idx, 3].imshow(true_mask, cmap='tab10', vmin=0, vmax=num_cd_classes-1)\n",
        "        axes[idx, 3].set_title('Ground Truth')\n",
        "        axes[idx, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_test_metrics(test_metrics, save_path):\n",
        "    \"\"\"Save test metrics to JSON\"\"\"\n",
        "    # Use the pre-computed metrics directly\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(test_metrics, f, indent=4)\n",
        "\n",
        "    print(f\"\\nSaved test metrics to: {save_path}\")\n",
        "\n",
        "# Test the model\n",
        "random_samples, test_metrics = test_model(model, test_loader, \n",
        "                                          device=device, num_cd_classes=num_cd_classes,\n",
        "                                          weighting_method=weighting_method,\n",
        "                                          weighted_metrics=weighted_metrics,\n",
        "                                          checkpoint_path=checkpoint_path)\n",
        "\n",
        "# Save test metrics\n",
        "save_test_metrics(test_metrics=test_metrics,\n",
        "                  save_path=f'{SAVING_DIR}/{name}_test_metrics.json')\n",
        "\n",
        "# Visualize results\n",
        "visualize_results(random_samples,num_cd_classes=num_cd_classes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5939633,
          "sourceId": 9710583,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
